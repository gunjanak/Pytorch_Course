{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pcPbm0Pqk24k",
        "outputId": "72a32017-f75b-4a98-90ff-20ad499ed658"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym==0.25.2 in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.25.2) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.25.2) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym==0.25.2) (0.0.8)\n",
            "Requirement already satisfied: swig in /usr/local/lib/python3.10/dist-packages (4.2.1)\n",
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (0.0.8)\n",
            "Requirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (2.3.5)\n",
            "Requirement already satisfied: pygame==2.1.0 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (2.1.0)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (4.2.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install gym==0.25.2\n",
        "!pip install swig\n",
        "!pip install gym[box2d]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import os"
      ],
      "metadata": {
        "id": "Gho7S1gYlHgw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = '/content/drive/MyDrive/Pytorch/rl/FQF'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7L2wq0b9nHkX",
        "outputId": "c48831ad-b314-473c-bc4b-5d50657e75cf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(PATH)"
      ],
      "metadata": {
        "id": "-b7-vlFHnNh7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trHanMaanPsd",
        "outputId": "9ef5e888-f78b-4422-8c67-19f999dfbd15"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Pytorch/rl/FQF\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define the FQF Network\n",
        "class FQFDQN(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, num_quantiles=51, hidden_dim=256):\n",
        "        super(FQFDQN, self).__init__()\n",
        "        self.num_quantiles = num_quantiles\n",
        "        self.action_dim = action_dim\n",
        "\n",
        "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.quantile_head = nn.Linear(hidden_dim, action_dim * num_quantiles)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        quantiles = self.quantile_head(x)\n",
        "        quantiles = quantiles.view(-1, self.action_dim, self.num_quantiles)\n",
        "        return quantiles\n"
      ],
      "metadata": {
        "id": "1JDgoSV7nSO9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_action(state, network, epsilon, action_dim, device):\n",
        "    if random.random() < epsilon:\n",
        "        return random.randrange(action_dim)\n",
        "    else:\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            quantiles = network(state)\n",
        "            mean_quantiles = quantiles.mean(dim=2)\n",
        "        return mean_quantiles.max(1)[1].item()\n",
        "\n",
        "def quantile_huber_loss(pred_quantiles, target_quantiles, taus, kappa=1.0):\n",
        "    diff = target_quantiles.unsqueeze(1) - pred_quantiles.unsqueeze(2)\n",
        "    huber_loss = torch.where(\n",
        "        torch.abs(diff) < kappa,\n",
        "        0.5 * diff ** 2,\n",
        "        kappa * (torch.abs(diff) - 0.5 * kappa)\n",
        "    )\n",
        "    taus = taus.unsqueeze(-1).expand_as(huber_loss)\n",
        "    loss = (taus - (diff < 0).float()).abs() * huber_loss\n",
        "    return loss.mean()\n",
        "\n",
        "def save_checkpoint(state, filename='checkpoint.pth'):\n",
        "    torch.save(state, filename)\n",
        "\n",
        "def load_checkpoint(filename='checkpoint.pth', map_location=None):\n",
        "    if map_location:\n",
        "        return torch.load(filename, map_location=map_location)\n",
        "    return torch.load(filename)"
      ],
      "metadata": {
        "id": "o_TgYdukqQx7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EZp45klm3dPz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "num_episodes = 1000\n",
        "batch_size = 64\n",
        "gamma = 0.99\n",
        "epsilon_start = 1.0\n",
        "epsilon_end = 0.01\n",
        "epsilon_decay = 0.995\n",
        "learning_rate = 1e-3\n",
        "target_update_steps = 1000\n",
        "num_quantiles = 51\n",
        "hidden_dim = 256"
      ],
      "metadata": {
        "id": "Fh-VkaDN3So5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('LunarLander-v2')\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4T1qryx23TIi",
        "outputId": "da736aef-98c4-4937-cc57-ee6c21cf7a3c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "network = FQFDQN(state_dim, action_dim, num_quantiles, hidden_dim).to(device)\n",
        "target_network = FQFDQN(state_dim, action_dim, num_quantiles, hidden_dim).to(device)"
      ],
      "metadata": {
        "id": "YQIOIdfA3tVF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(network.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "U2g3PCXY4EN-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = 'fqf.pth'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYtbry063tvJ",
        "outputId": "e47eeefc-82eb-4a4f-cb70-00c44e7450ba"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    map_location = torch.device('cpu') if not torch.cuda.is_available() else None\n",
        "    checkpoint = load_checkpoint(checkpoint_path, map_location=map_location)\n",
        "    network.load_state_dict(checkpoint['main_net_state_dict'])\n",
        "    target_network.load_state_dict(checkpoint['target_net_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epsilon = checkpoint['epsilon']\n",
        "    start_episode = checkpoint['episode'] + 1\n",
        "    print(f\"Loaded checkpoint from episode {start_episode}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"No checkpoint found, starting from scratch.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kb00q0kJ38FZ",
        "outputId": "edc82077-24f1-4497-d01b-425d6bd1d993"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No checkpoint found, starting from scratch.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "target_network.load_state_dict(network.state_dict())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVmdZxRC388x",
        "outputId": "d126d512-8ed2-4a0f-9f34-c107474cd898"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "replay_buffer = deque(maxlen=10000)\n",
        "epsilon = epsilon_start\n",
        "total_steps = 0\n",
        "episode_rewards = []\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    episode_reward = 0\n",
        "\n",
        "    while True:\n",
        "        action = select_action(state, network, epsilon, action_dim, device)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        replay_buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "        if len(replay_buffer) > batch_size:\n",
        "            batch = random.sample(replay_buffer, batch_size)\n",
        "            states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "            states = torch.FloatTensor(states).to(device)\n",
        "            actions = torch.LongTensor(actions).to(device)\n",
        "            rewards = torch.FloatTensor(rewards).to(device)\n",
        "            next_states = torch.FloatTensor(next_states).to(device)\n",
        "            dones = torch.FloatTensor(dones).to(device)\n",
        "\n",
        "            quantiles = network(states)\n",
        "            actions = actions.unsqueeze(1).unsqueeze(2).expand(batch_size, 1, num_quantiles)\n",
        "            quantiles = quantiles.gather(1, actions).squeeze(1)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                next_quantiles = target_network(next_states)\n",
        "                next_actions = next_quantiles.mean(dim=2).max(1)[1]\n",
        "                next_actions = next_actions.unsqueeze(1).unsqueeze(2).expand(batch_size, 1, num_quantiles)\n",
        "                next_target_quantiles = next_quantiles.gather(1, next_actions).squeeze(1)\n",
        "                target_quantiles = rewards.unsqueeze(1) + gamma * (1 - dones.unsqueeze(1)) * next_target_quantiles\n",
        "\n",
        "            taus = torch.rand(batch_size, num_quantiles).to(device)\n",
        "            loss = quantile_huber_loss(quantiles, target_quantiles, taus)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(network.parameters(), 1.0)  # Gradient clipping\n",
        "            optimizer.step()\n",
        "\n",
        "        state = next_state\n",
        "        episode_reward += reward\n",
        "        total_steps += 1\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "        if total_steps % target_update_steps == 0:\n",
        "            target_network.load_state_dict(network.state_dict())\n",
        "\n",
        "    epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
        "    # Logging and monitoring\n",
        "    episode_rewards.append(episode_reward)\n",
        "    print(f\"Episode {episode + 1}, Reward: {episode_reward}, Epsilon: {epsilon:.2f}\")\n",
        "\n",
        "    # Save model periodically\n",
        "    if episode % 50 == 0:\n",
        "        save_checkpoint({\n",
        "            'episode': episode,\n",
        "            'main_net_state_dict': network.state_dict(),\n",
        "            'target_net_state_dict': target_network.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'epsilon': epsilon\n",
        "        },checkpoint_path)\n",
        "        print(f\"Checkpoint saved at episode {episode}\")\n",
        "\n",
        "\n",
        "    # Early stopping condition\n",
        "    if sum(episode_rewards[-5:]) > 1000:\n",
        "        print(\"Training done\")\n",
        "        save_checkpoint({\n",
        "            'episode': episode,\n",
        "            'main_net_state_dict': network.state_dict(),\n",
        "            'target_net_state_dict': target_network.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'epsilon': epsilon\n",
        "        },checkpoint_path)\n",
        "        print(f\"Checkpoint saved at episode {episode}\")\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Kwtu32hqTDQ",
        "outputId": "dc3d9bda-4dab-45a7-f836-1eec5889d5bb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n",
            "<ipython-input-15-831bf7e3d24c>:19: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
            "  states = torch.FloatTensor(states).to(device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1, Reward: -95.08900329208883, Epsilon: 0.99\n",
            "Checkpoint saved at episode 0\n",
            "Episode 2, Reward: -523.5931071922687, Epsilon: 0.99\n",
            "Episode 3, Reward: -112.40846539255574, Epsilon: 0.99\n",
            "Episode 4, Reward: 82.0524337492685, Epsilon: 0.98\n",
            "Episode 5, Reward: -290.1639843312145, Epsilon: 0.98\n",
            "Episode 6, Reward: -187.79072942766783, Epsilon: 0.97\n",
            "Episode 7, Reward: -58.93888695483702, Epsilon: 0.97\n",
            "Episode 8, Reward: -123.25033492164492, Epsilon: 0.96\n",
            "Episode 9, Reward: -334.2461458971462, Epsilon: 0.96\n",
            "Episode 10, Reward: -44.34722865072406, Epsilon: 0.95\n",
            "Episode 11, Reward: -389.44943607561567, Epsilon: 0.95\n",
            "Episode 12, Reward: -68.37985559047175, Epsilon: 0.94\n",
            "Episode 13, Reward: -401.55595302330056, Epsilon: 0.94\n",
            "Episode 14, Reward: -144.27076314964893, Epsilon: 0.93\n",
            "Episode 15, Reward: -262.0000517320025, Epsilon: 0.93\n",
            "Episode 16, Reward: -85.17927355132683, Epsilon: 0.92\n",
            "Episode 17, Reward: -153.64137043627505, Epsilon: 0.92\n",
            "Episode 18, Reward: -95.70030688169854, Epsilon: 0.91\n",
            "Episode 19, Reward: -96.26210984513185, Epsilon: 0.91\n",
            "Episode 20, Reward: -79.86791950624433, Epsilon: 0.90\n",
            "Episode 21, Reward: -143.50909784761345, Epsilon: 0.90\n",
            "Episode 22, Reward: -67.30866074817962, Epsilon: 0.90\n",
            "Episode 23, Reward: -71.812966594562, Epsilon: 0.89\n",
            "Episode 24, Reward: -118.91354795295044, Epsilon: 0.89\n",
            "Episode 25, Reward: -103.12935734527115, Epsilon: 0.88\n",
            "Episode 26, Reward: -67.10786976447837, Epsilon: 0.88\n",
            "Episode 27, Reward: -160.93600036059308, Epsilon: 0.87\n",
            "Episode 28, Reward: -108.09391468414498, Epsilon: 0.87\n",
            "Episode 29, Reward: -208.03965212692606, Epsilon: 0.86\n",
            "Episode 30, Reward: -104.4800302897265, Epsilon: 0.86\n",
            "Episode 31, Reward: -106.1857269933286, Epsilon: 0.86\n",
            "Episode 32, Reward: -194.60635899914686, Epsilon: 0.85\n",
            "Episode 33, Reward: -177.27942080913095, Epsilon: 0.85\n",
            "Episode 34, Reward: -97.18753997129802, Epsilon: 0.84\n",
            "Episode 35, Reward: -15.557376408135, Epsilon: 0.84\n",
            "Episode 36, Reward: -157.55213733029322, Epsilon: 0.83\n",
            "Episode 37, Reward: -56.305416875684905, Epsilon: 0.83\n",
            "Episode 38, Reward: -117.30743255194486, Epsilon: 0.83\n",
            "Episode 39, Reward: -104.82615400769781, Epsilon: 0.82\n",
            "Episode 40, Reward: -88.52100360119375, Epsilon: 0.82\n",
            "Episode 41, Reward: -126.6676692926392, Epsilon: 0.81\n",
            "Episode 42, Reward: -150.58428426411893, Epsilon: 0.81\n",
            "Episode 43, Reward: -105.56595719295274, Epsilon: 0.81\n",
            "Episode 44, Reward: -82.15657375337716, Epsilon: 0.80\n",
            "Episode 45, Reward: -203.80020953906265, Epsilon: 0.80\n",
            "Episode 46, Reward: -130.43423490634262, Epsilon: 0.79\n",
            "Episode 47, Reward: -222.64011819224487, Epsilon: 0.79\n",
            "Episode 48, Reward: -129.20540374411752, Epsilon: 0.79\n",
            "Episode 49, Reward: -94.52760984295628, Epsilon: 0.78\n",
            "Episode 50, Reward: -21.17575995866038, Epsilon: 0.78\n",
            "Episode 51, Reward: 5.320360739873351, Epsilon: 0.77\n",
            "Checkpoint saved at episode 50\n",
            "Episode 52, Reward: -259.53619003165613, Epsilon: 0.77\n",
            "Episode 53, Reward: -32.43575063930466, Epsilon: 0.77\n",
            "Episode 54, Reward: -253.5443459377996, Epsilon: 0.76\n",
            "Episode 55, Reward: -55.62795582229117, Epsilon: 0.76\n",
            "Episode 56, Reward: -101.92941366759324, Epsilon: 0.76\n",
            "Episode 57, Reward: -143.89867805366737, Epsilon: 0.75\n",
            "Episode 58, Reward: -120.30102409188129, Epsilon: 0.75\n",
            "Episode 59, Reward: -75.41727569125159, Epsilon: 0.74\n",
            "Episode 60, Reward: -56.223099644785805, Epsilon: 0.74\n",
            "Episode 61, Reward: -65.44679340438078, Epsilon: 0.74\n",
            "Episode 62, Reward: -87.84501998212743, Epsilon: 0.73\n",
            "Episode 63, Reward: -86.01886968630262, Epsilon: 0.73\n",
            "Episode 64, Reward: -79.49589824528564, Epsilon: 0.73\n",
            "Episode 65, Reward: -265.8881136079507, Epsilon: 0.72\n",
            "Episode 66, Reward: -112.8404442661901, Epsilon: 0.72\n",
            "Episode 67, Reward: -91.11526473290485, Epsilon: 0.71\n",
            "Episode 68, Reward: -67.52434594875518, Epsilon: 0.71\n",
            "Episode 69, Reward: -102.45891758873665, Epsilon: 0.71\n",
            "Episode 70, Reward: -86.00187344485809, Epsilon: 0.70\n",
            "Episode 71, Reward: -86.15930794755585, Epsilon: 0.70\n",
            "Episode 72, Reward: -64.04834840787001, Epsilon: 0.70\n",
            "Episode 73, Reward: -128.96044334292952, Epsilon: 0.69\n",
            "Episode 74, Reward: 9.309886273290218, Epsilon: 0.69\n",
            "Episode 75, Reward: -100.69990381491985, Epsilon: 0.69\n",
            "Episode 76, Reward: -90.81800777284509, Epsilon: 0.68\n",
            "Episode 77, Reward: -171.62253368629484, Epsilon: 0.68\n",
            "Episode 78, Reward: -95.9620077595277, Epsilon: 0.68\n",
            "Episode 79, Reward: -9.475932398116683, Epsilon: 0.67\n",
            "Episode 80, Reward: -74.02310222823596, Epsilon: 0.67\n",
            "Episode 81, Reward: -35.300213754308956, Epsilon: 0.67\n",
            "Episode 82, Reward: -67.47736040266474, Epsilon: 0.66\n",
            "Episode 83, Reward: -62.35821633345468, Epsilon: 0.66\n",
            "Episode 84, Reward: -80.99100191780786, Epsilon: 0.66\n",
            "Episode 85, Reward: -78.67186638125978, Epsilon: 0.65\n",
            "Episode 86, Reward: -60.26833883575961, Epsilon: 0.65\n",
            "Episode 87, Reward: -41.512087063665966, Epsilon: 0.65\n",
            "Episode 88, Reward: -8.29511892020163, Epsilon: 0.64\n",
            "Episode 89, Reward: -42.18811334086232, Epsilon: 0.64\n",
            "Episode 90, Reward: -42.11435627232035, Epsilon: 0.64\n",
            "Episode 91, Reward: -76.5334690145491, Epsilon: 0.63\n",
            "Episode 92, Reward: -59.979538536833246, Epsilon: 0.63\n",
            "Episode 93, Reward: -103.67691543484383, Epsilon: 0.63\n",
            "Episode 94, Reward: -30.21027423195538, Epsilon: 0.62\n",
            "Episode 95, Reward: -48.70276543211821, Epsilon: 0.62\n",
            "Episode 96, Reward: -86.68749486869328, Epsilon: 0.62\n",
            "Episode 97, Reward: -56.63581341554647, Epsilon: 0.61\n",
            "Episode 98, Reward: -64.64483851406888, Epsilon: 0.61\n",
            "Episode 99, Reward: -52.01092331864044, Epsilon: 0.61\n",
            "Episode 100, Reward: -101.15425424254107, Epsilon: 0.61\n",
            "Episode 101, Reward: 9.063398859858395, Epsilon: 0.60\n",
            "Checkpoint saved at episode 100\n",
            "Episode 102, Reward: -25.725374961674873, Epsilon: 0.60\n",
            "Episode 103, Reward: -39.44467073886206, Epsilon: 0.60\n",
            "Episode 104, Reward: -71.07078039764923, Epsilon: 0.59\n",
            "Episode 105, Reward: -86.07514164067868, Epsilon: 0.59\n",
            "Episode 106, Reward: -95.30096249364266, Epsilon: 0.59\n",
            "Episode 107, Reward: -51.23859277721552, Epsilon: 0.58\n",
            "Episode 108, Reward: -116.18739901183339, Epsilon: 0.58\n",
            "Episode 109, Reward: -50.89184900704254, Epsilon: 0.58\n",
            "Episode 110, Reward: -97.16804177034341, Epsilon: 0.58\n",
            "Episode 111, Reward: -9.932974891685973, Epsilon: 0.57\n",
            "Episode 112, Reward: 20.904206377156527, Epsilon: 0.57\n",
            "Episode 113, Reward: -162.3335387129635, Epsilon: 0.57\n",
            "Episode 114, Reward: 28.548080847122208, Epsilon: 0.56\n",
            "Episode 115, Reward: -47.14480836897559, Epsilon: 0.56\n",
            "Episode 116, Reward: -52.84708687225981, Epsilon: 0.56\n",
            "Episode 117, Reward: -12.348577147761915, Epsilon: 0.56\n",
            "Episode 118, Reward: -76.60763969659565, Epsilon: 0.55\n",
            "Episode 119, Reward: -70.39950540655028, Epsilon: 0.55\n",
            "Episode 120, Reward: -44.89985932999356, Epsilon: 0.55\n",
            "Episode 121, Reward: 100.37959410241584, Epsilon: 0.55\n",
            "Episode 122, Reward: -92.21335609602288, Epsilon: 0.54\n",
            "Episode 123, Reward: -83.72638467846855, Epsilon: 0.54\n",
            "Episode 124, Reward: -35.540346494846474, Epsilon: 0.54\n",
            "Episode 125, Reward: -10.337033605935261, Epsilon: 0.53\n",
            "Episode 126, Reward: -36.371840614079886, Epsilon: 0.53\n",
            "Episode 127, Reward: -67.7192853143604, Epsilon: 0.53\n",
            "Episode 128, Reward: -213.71967122347473, Epsilon: 0.53\n",
            "Episode 129, Reward: -3.4175202626728236, Epsilon: 0.52\n",
            "Episode 130, Reward: -60.18826104485143, Epsilon: 0.52\n",
            "Episode 131, Reward: 44.719637403751804, Epsilon: 0.52\n",
            "Episode 132, Reward: -8.107276040741894, Epsilon: 0.52\n",
            "Episode 133, Reward: 108.67679949157306, Epsilon: 0.51\n",
            "Episode 134, Reward: -26.708898712137184, Epsilon: 0.51\n",
            "Episode 135, Reward: -67.39689050651444, Epsilon: 0.51\n",
            "Episode 136, Reward: -57.47783177201008, Epsilon: 0.51\n",
            "Episode 137, Reward: 1.6769349794544155, Epsilon: 0.50\n",
            "Episode 138, Reward: -55.173002349493686, Epsilon: 0.50\n",
            "Episode 139, Reward: 84.12555760110098, Epsilon: 0.50\n",
            "Episode 140, Reward: -38.88244306311482, Epsilon: 0.50\n",
            "Episode 141, Reward: 113.29884912473102, Epsilon: 0.49\n",
            "Episode 142, Reward: -62.824758359582404, Epsilon: 0.49\n",
            "Episode 143, Reward: -12.202998725884825, Epsilon: 0.49\n",
            "Episode 144, Reward: 14.098159158100387, Epsilon: 0.49\n",
            "Episode 145, Reward: -253.81748251375257, Epsilon: 0.48\n",
            "Episode 146, Reward: 112.54998511678183, Epsilon: 0.48\n",
            "Episode 147, Reward: -47.35001641657162, Epsilon: 0.48\n",
            "Episode 148, Reward: -8.22290460939405, Epsilon: 0.48\n",
            "Episode 149, Reward: 44.74530936473674, Epsilon: 0.47\n",
            "Episode 150, Reward: 77.76868365786743, Epsilon: 0.47\n",
            "Episode 151, Reward: -71.04404007614745, Epsilon: 0.47\n",
            "Checkpoint saved at episode 150\n",
            "Episode 152, Reward: 7.117399298136746, Epsilon: 0.47\n",
            "Episode 153, Reward: 137.95863338055565, Epsilon: 0.46\n",
            "Episode 154, Reward: -60.8603204366415, Epsilon: 0.46\n",
            "Episode 155, Reward: -57.645400122040854, Epsilon: 0.46\n",
            "Episode 156, Reward: -19.895094936924536, Epsilon: 0.46\n",
            "Episode 157, Reward: 92.58700310275836, Epsilon: 0.46\n",
            "Episode 158, Reward: 5.611331081338534, Epsilon: 0.45\n",
            "Episode 159, Reward: -61.526802973920795, Epsilon: 0.45\n",
            "Episode 160, Reward: -36.17998317550297, Epsilon: 0.45\n",
            "Episode 161, Reward: -24.08786980860839, Epsilon: 0.45\n",
            "Episode 162, Reward: 35.90158243520585, Epsilon: 0.44\n",
            "Episode 163, Reward: 37.48112230469877, Epsilon: 0.44\n",
            "Episode 164, Reward: -0.5681225407382442, Epsilon: 0.44\n",
            "Episode 165, Reward: 82.78807259592041, Epsilon: 0.44\n",
            "Episode 166, Reward: 30.238104187920783, Epsilon: 0.44\n",
            "Episode 167, Reward: -97.78746647119061, Epsilon: 0.43\n",
            "Episode 168, Reward: -6.395477195608905, Epsilon: 0.43\n",
            "Episode 169, Reward: -1.0333801959765196, Epsilon: 0.43\n",
            "Episode 170, Reward: -138.16414653194127, Epsilon: 0.43\n",
            "Episode 171, Reward: -12.453785524237489, Epsilon: 0.42\n",
            "Episode 172, Reward: -60.59582274261235, Epsilon: 0.42\n",
            "Episode 173, Reward: 15.225929024832595, Epsilon: 0.42\n",
            "Episode 174, Reward: 18.575181753196674, Epsilon: 0.42\n",
            "Episode 175, Reward: -62.58855829566561, Epsilon: 0.42\n",
            "Episode 176, Reward: 42.78197159361156, Epsilon: 0.41\n",
            "Episode 177, Reward: -27.25586884395983, Epsilon: 0.41\n",
            "Episode 178, Reward: -24.212051060113964, Epsilon: 0.41\n",
            "Episode 179, Reward: 101.41569157189706, Epsilon: 0.41\n",
            "Episode 180, Reward: -8.97376748341226, Epsilon: 0.41\n",
            "Episode 181, Reward: -39.61220381853376, Epsilon: 0.40\n",
            "Episode 182, Reward: 16.004818575155582, Epsilon: 0.40\n",
            "Episode 183, Reward: -45.7151457743182, Epsilon: 0.40\n",
            "Episode 184, Reward: 65.02499096330672, Epsilon: 0.40\n",
            "Episode 185, Reward: -10.482663521283541, Epsilon: 0.40\n",
            "Episode 186, Reward: -3.9294532658530272, Epsilon: 0.39\n",
            "Episode 187, Reward: -6.91288175228469, Epsilon: 0.39\n",
            "Episode 188, Reward: 12.899191105867786, Epsilon: 0.39\n",
            "Episode 189, Reward: 56.36077760629452, Epsilon: 0.39\n",
            "Episode 190, Reward: 2.802305819151755, Epsilon: 0.39\n",
            "Episode 191, Reward: 131.32641278991105, Epsilon: 0.38\n",
            "Episode 192, Reward: -22.992168853744076, Epsilon: 0.38\n",
            "Episode 193, Reward: 24.026243584202092, Epsilon: 0.38\n",
            "Episode 194, Reward: 30.738656350807588, Epsilon: 0.38\n",
            "Episode 195, Reward: 10.401928688274197, Epsilon: 0.38\n",
            "Episode 196, Reward: -15.736213278371864, Epsilon: 0.37\n",
            "Episode 197, Reward: 7.89323320044582, Epsilon: 0.37\n",
            "Episode 198, Reward: 128.68738586899923, Epsilon: 0.37\n",
            "Episode 199, Reward: 106.16182339679129, Epsilon: 0.37\n",
            "Episode 200, Reward: 3.4874899827733827, Epsilon: 0.37\n",
            "Episode 201, Reward: -38.81277086552832, Epsilon: 0.37\n",
            "Checkpoint saved at episode 200\n",
            "Episode 202, Reward: 12.971958821760353, Epsilon: 0.36\n",
            "Episode 203, Reward: -40.629755250385855, Epsilon: 0.36\n",
            "Episode 204, Reward: 2.9026850621411597, Epsilon: 0.36\n",
            "Episode 205, Reward: -22.58463075894305, Epsilon: 0.36\n",
            "Episode 206, Reward: -11.202998141031841, Epsilon: 0.36\n",
            "Episode 207, Reward: -191.88761750492648, Epsilon: 0.35\n",
            "Episode 208, Reward: 130.39847009401427, Epsilon: 0.35\n",
            "Episode 209, Reward: -41.22800051656142, Epsilon: 0.35\n",
            "Episode 210, Reward: 0.7340755526748524, Epsilon: 0.35\n",
            "Episode 211, Reward: 111.77308730779646, Epsilon: 0.35\n",
            "Episode 212, Reward: 35.77935754720346, Epsilon: 0.35\n",
            "Episode 213, Reward: 25.9355330601161, Epsilon: 0.34\n",
            "Episode 214, Reward: 47.57078106273651, Epsilon: 0.34\n",
            "Episode 215, Reward: -20.7612801323873, Epsilon: 0.34\n",
            "Episode 216, Reward: 25.092244077615902, Epsilon: 0.34\n",
            "Episode 217, Reward: 2.0968489772409384, Epsilon: 0.34\n",
            "Episode 218, Reward: 134.11284825242038, Epsilon: 0.34\n",
            "Episode 219, Reward: 0.8498121167692574, Epsilon: 0.33\n",
            "Episode 220, Reward: -5.010357984214465, Epsilon: 0.33\n",
            "Episode 221, Reward: 53.1330126850535, Epsilon: 0.33\n",
            "Episode 222, Reward: 47.65357768978549, Epsilon: 0.33\n",
            "Episode 223, Reward: 26.94504524083949, Epsilon: 0.33\n",
            "Episode 224, Reward: -40.76334672320971, Epsilon: 0.33\n",
            "Episode 225, Reward: 17.690544032606027, Epsilon: 0.32\n",
            "Episode 226, Reward: 262.3829241142513, Epsilon: 0.32\n",
            "Episode 227, Reward: -22.529253266216557, Epsilon: 0.32\n",
            "Episode 228, Reward: 119.97958295391996, Epsilon: 0.32\n",
            "Episode 229, Reward: 16.774525272207853, Epsilon: 0.32\n",
            "Episode 230, Reward: 11.650391327517625, Epsilon: 0.32\n",
            "Episode 231, Reward: -6.907978042641844, Epsilon: 0.31\n",
            "Episode 232, Reward: 30.60789835625482, Epsilon: 0.31\n",
            "Episode 233, Reward: 100.32329505246273, Epsilon: 0.31\n",
            "Episode 234, Reward: 92.29880704314786, Epsilon: 0.31\n",
            "Episode 235, Reward: 2.173499373904974, Epsilon: 0.31\n",
            "Episode 236, Reward: 150.62592019309153, Epsilon: 0.31\n",
            "Episode 237, Reward: 15.545550163493147, Epsilon: 0.30\n",
            "Episode 238, Reward: 23.749391159813257, Epsilon: 0.30\n",
            "Episode 239, Reward: -28.42184880725796, Epsilon: 0.30\n",
            "Episode 240, Reward: 13.379724047014548, Epsilon: 0.30\n",
            "Episode 241, Reward: 34.863234893285295, Epsilon: 0.30\n",
            "Episode 242, Reward: 5.10878666446699, Epsilon: 0.30\n",
            "Episode 243, Reward: 20.104299192794997, Epsilon: 0.30\n",
            "Episode 244, Reward: -135.78927145417506, Epsilon: 0.29\n",
            "Episode 245, Reward: 100.48488767697792, Epsilon: 0.29\n",
            "Episode 246, Reward: 48.25106226063971, Epsilon: 0.29\n",
            "Episode 247, Reward: 8.348130455474115, Epsilon: 0.29\n",
            "Episode 248, Reward: -13.056120599407905, Epsilon: 0.29\n",
            "Episode 249, Reward: -51.52069381631037, Epsilon: 0.29\n",
            "Episode 250, Reward: 7.060448177098536, Epsilon: 0.29\n",
            "Episode 251, Reward: 20.80651684545009, Epsilon: 0.28\n",
            "Checkpoint saved at episode 250\n",
            "Episode 252, Reward: 13.204483801819322, Epsilon: 0.28\n",
            "Episode 253, Reward: 30.0946695871053, Epsilon: 0.28\n",
            "Episode 254, Reward: -17.07211791338851, Epsilon: 0.28\n",
            "Episode 255, Reward: 16.685021952000923, Epsilon: 0.28\n",
            "Episode 256, Reward: 273.9421993265456, Epsilon: 0.28\n",
            "Episode 257, Reward: -4.722033782591183, Epsilon: 0.28\n",
            "Episode 258, Reward: 15.19062230243427, Epsilon: 0.27\n",
            "Episode 259, Reward: 235.78929423162663, Epsilon: 0.27\n",
            "Episode 260, Reward: 6.167112170078681, Epsilon: 0.27\n",
            "Episode 261, Reward: 144.49843557857128, Epsilon: 0.27\n",
            "Episode 262, Reward: 106.86454889237783, Epsilon: 0.27\n",
            "Episode 263, Reward: -2.456814311505269, Epsilon: 0.27\n",
            "Episode 264, Reward: -4.514280533049629, Epsilon: 0.27\n",
            "Episode 265, Reward: -149.3587402349666, Epsilon: 0.26\n",
            "Episode 266, Reward: 19.621687261361544, Epsilon: 0.26\n",
            "Episode 267, Reward: -73.4900011294664, Epsilon: 0.26\n",
            "Episode 268, Reward: 22.896195420201053, Epsilon: 0.26\n",
            "Episode 269, Reward: 37.64380098437866, Epsilon: 0.26\n",
            "Episode 270, Reward: 18.289489077476105, Epsilon: 0.26\n",
            "Episode 271, Reward: 5.097726136401235, Epsilon: 0.26\n",
            "Episode 272, Reward: 129.16552697086337, Epsilon: 0.26\n",
            "Episode 273, Reward: 17.496551868756725, Epsilon: 0.25\n",
            "Episode 274, Reward: -2.5309965479777787, Epsilon: 0.25\n",
            "Episode 275, Reward: 24.122935027692392, Epsilon: 0.25\n",
            "Episode 276, Reward: 257.68141632668386, Epsilon: 0.25\n",
            "Episode 277, Reward: 16.969191192963322, Epsilon: 0.25\n",
            "Episode 278, Reward: 20.614038139906995, Epsilon: 0.25\n",
            "Episode 279, Reward: 41.91648546456844, Epsilon: 0.25\n",
            "Episode 280, Reward: 69.27460264110695, Epsilon: 0.25\n",
            "Episode 281, Reward: 142.45973084657993, Epsilon: 0.24\n",
            "Episode 282, Reward: 75.0811810512835, Epsilon: 0.24\n",
            "Episode 283, Reward: 32.205377647307955, Epsilon: 0.24\n",
            "Episode 284, Reward: -27.515369911733018, Epsilon: 0.24\n",
            "Episode 285, Reward: 143.3967266732714, Epsilon: 0.24\n",
            "Episode 286, Reward: 23.902311052183407, Epsilon: 0.24\n",
            "Episode 287, Reward: 25.828283858470414, Epsilon: 0.24\n",
            "Episode 288, Reward: 156.5196686556363, Epsilon: 0.24\n",
            "Episode 289, Reward: 21.148321517572526, Epsilon: 0.23\n",
            "Episode 290, Reward: 51.43129662181531, Epsilon: 0.23\n",
            "Episode 291, Reward: 155.38464814488628, Epsilon: 0.23\n",
            "Episode 292, Reward: -29.00554949369831, Epsilon: 0.23\n",
            "Episode 293, Reward: -36.69438676383977, Epsilon: 0.23\n",
            "Episode 294, Reward: -72.56474835216596, Epsilon: 0.23\n",
            "Episode 295, Reward: 0.7803663243104069, Epsilon: 0.23\n",
            "Episode 296, Reward: 251.533720296433, Epsilon: 0.23\n",
            "Episode 297, Reward: 157.12702221114614, Epsilon: 0.23\n",
            "Episode 298, Reward: 0.7357871546129076, Epsilon: 0.22\n",
            "Episode 299, Reward: -26.753176689103185, Epsilon: 0.22\n",
            "Episode 300, Reward: 20.730612568651324, Epsilon: 0.22\n",
            "Episode 301, Reward: 126.52981376169488, Epsilon: 0.22\n",
            "Checkpoint saved at episode 300\n",
            "Episode 302, Reward: 39.96195770389676, Epsilon: 0.22\n",
            "Episode 303, Reward: 92.4907950612374, Epsilon: 0.22\n",
            "Episode 304, Reward: -11.508935891208068, Epsilon: 0.22\n",
            "Episode 305, Reward: -127.8455065143787, Epsilon: 0.22\n",
            "Episode 306, Reward: -19.37193749693182, Epsilon: 0.22\n",
            "Episode 307, Reward: 21.89151304787947, Epsilon: 0.21\n",
            "Episode 308, Reward: 112.74181076857906, Epsilon: 0.21\n",
            "Episode 309, Reward: 19.382912707265262, Epsilon: 0.21\n",
            "Episode 310, Reward: -73.65227618853775, Epsilon: 0.21\n",
            "Episode 311, Reward: 76.65108063937805, Epsilon: 0.21\n",
            "Episode 312, Reward: 225.82013620825734, Epsilon: 0.21\n",
            "Episode 313, Reward: 168.99932782633323, Epsilon: 0.21\n",
            "Episode 314, Reward: 49.76767764282573, Epsilon: 0.21\n",
            "Episode 315, Reward: 8.194136375296708, Epsilon: 0.21\n",
            "Episode 316, Reward: -319.58955439434584, Epsilon: 0.21\n",
            "Episode 317, Reward: -259.911209485302, Epsilon: 0.20\n",
            "Episode 318, Reward: -6.9245797206368165, Epsilon: 0.20\n",
            "Episode 319, Reward: -19.59619335095597, Epsilon: 0.20\n",
            "Episode 320, Reward: -340.9318173082688, Epsilon: 0.20\n",
            "Episode 321, Reward: 23.77150723466997, Epsilon: 0.20\n",
            "Episode 322, Reward: -33.730870565373266, Epsilon: 0.20\n",
            "Episode 323, Reward: -346.57496006157453, Epsilon: 0.20\n",
            "Episode 324, Reward: -24.298723730508087, Epsilon: 0.20\n",
            "Episode 325, Reward: 57.62817985493887, Epsilon: 0.20\n",
            "Episode 326, Reward: 7.236295086175744, Epsilon: 0.20\n",
            "Episode 327, Reward: -131.36854690994602, Epsilon: 0.19\n",
            "Episode 328, Reward: 18.34886204963638, Epsilon: 0.19\n",
            "Episode 329, Reward: 251.8936940119113, Epsilon: 0.19\n",
            "Episode 330, Reward: 24.733307749991297, Epsilon: 0.19\n",
            "Episode 331, Reward: -266.94503732846636, Epsilon: 0.19\n",
            "Episode 332, Reward: 8.263853478855637, Epsilon: 0.19\n",
            "Episode 333, Reward: -14.373978653827237, Epsilon: 0.19\n",
            "Episode 334, Reward: 19.39226755718363, Epsilon: 0.19\n",
            "Episode 335, Reward: 117.52392627524969, Epsilon: 0.19\n",
            "Episode 336, Reward: -38.36259811678882, Epsilon: 0.19\n",
            "Episode 337, Reward: 264.026629852416, Epsilon: 0.18\n",
            "Episode 338, Reward: -24.629392704373647, Epsilon: 0.18\n",
            "Episode 339, Reward: 21.190863152207925, Epsilon: 0.18\n",
            "Episode 340, Reward: 261.6044578489427, Epsilon: 0.18\n",
            "Episode 341, Reward: 277.0224018547792, Epsilon: 0.18\n",
            "Episode 342, Reward: -1.898945515430114, Epsilon: 0.18\n",
            "Episode 343, Reward: 244.6193476667173, Epsilon: 0.18\n",
            "Episode 344, Reward: 269.29187045614094, Epsilon: 0.18\n",
            "Training done\n",
            "Checkpoint saved at episode 343\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SD4AhMBJqYmz"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}