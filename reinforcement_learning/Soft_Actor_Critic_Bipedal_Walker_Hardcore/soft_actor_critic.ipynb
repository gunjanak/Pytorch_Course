{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gym version: 0.25.2\n",
      "Torch version: 2.2.2+cpu\n",
      "Numpy version: 1.26.3\n",
      "24\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janak/Documents/Pytorch_CPU/venv/lib/python3.10/site-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import resource_stream, resource_exists\n",
      "/home/janak/Documents/Pytorch_CPU/venv/lib/python3.10/site-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/home/janak/Documents/Pytorch_CPU/venv/lib/python3.10/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "# from colabgymrender.recorder import Recorder\n",
    "\n",
    "# Print versions of gym, torch, and numpy\n",
    "print(f\"Gym version: {gym.__version__}\")\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"Numpy version: {np.__version__}\")\n",
    "\n",
    "\n",
    "env = gym.make('BipedalWalker-v3', hardcore=True)\n",
    "# env = gym.make('BipedalWalker-v3')\n",
    "s_dim = env.observation_space.shape[0]\n",
    "a_dim = env.action_space.shape[0]\n",
    "print(s_dim)\n",
    "print(a_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributions as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "# https://alexandervandekleut.github.io/gym-wrappers/\n",
    "\n",
    "class MyWalkerWrapper(gym.Wrapper):\n",
    "    '''\n",
    "    This is custom wrapper for BipedalWalker-v3 and BipedalWalkerHardcore-v3.\n",
    "    Rewards for failure is decreased to make agent brave for exploration and\n",
    "    time frequency of dynamic is lowered by skipping two frames.\n",
    "    '''\n",
    "    def __init__(self, env, skip=2):\n",
    "        super().__init__(env)\n",
    "        self._obs_buffer = deque(maxlen=skip)\n",
    "        self._skip = skip\n",
    "        self._max_episode_steps = 1200\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            if self.env.game_over:\n",
    "                reward = -10.0\n",
    "                info[\"dead\"] = True\n",
    "            else:\n",
    "            \tinfo[\"dead\"] = False\n",
    "\n",
    "            self._obs_buffer.append(obs)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        return obs, total_reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        return self.env.reset()\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        for _ in range(self._skip):\n",
    "            out = self.env.render(mode=mode)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MyWalkerWrapper(env, skip=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque, namedtuple\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Simle experience replay buffer for deep reinforcement algorithms.\"\"\"\n",
    "    def __init__(self, action_size, buffer_size, batch_size, device):\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        \n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.stack([e.state for e in experiences if e is not None], axis=0)).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.stack([e.action for e in experiences if e is not None], axis=0)).float().to(self.device)\n",
    "        rewards = torch.from_numpy(np.stack([e.reward for e in experiences if e is not None], axis=0)).float().unsqueeze(-1).to(self.device)\n",
    "        next_states = torch.from_numpy(np.stack([e.next_state for e in experiences if e is not None], axis=0)).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.stack([e.done for e in experiences if e is not None], axis=0).astype(np.uint8)).float().unsqueeze(-1).to(self.device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "class SACAgent():\n",
    "    rl_type = 'sac'\n",
    "    def __init__(self, Actor, Critic, clip_low, clip_high, state_size=24, action_size=4, update_freq=int(1),\n",
    "            lr=4e-4, weight_decay=0, gamma=0.98, alpha=0.01, tau=0.01, batch_size=64, buffer_size=int(500000), device=None):\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.update_freq = update_freq\n",
    "\n",
    "        self.learn_call = int(0)\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        if device is None:\n",
    "            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "\n",
    "        self.clip_low = torch.tensor(clip_low)\n",
    "        self.clip_high = torch.tensor(clip_high)\n",
    "\n",
    "        self.train_actor = Actor(stochastic=True).to(self.device)\n",
    "        self.actor_optim = torch.optim.AdamW(self.train_actor.parameters(), lr=lr, weight_decay=weight_decay, amsgrad=True)\n",
    "        print(f'Number of paramters of Actor Net: {sum(p.numel() for p in self.train_actor.parameters())}')\n",
    "        \n",
    "        self.train_critic_1 = Critic().to(self.device)\n",
    "        self.target_critic_1 = Critic().to(self.device).eval()\n",
    "        self.hard_update(self.train_critic_1, self.target_critic_1) # hard update at the beginning\n",
    "        self.critic_1_optim = torch.optim.AdamW(self.train_critic_1.parameters(), lr=lr, weight_decay=weight_decay, amsgrad=True)\n",
    "\n",
    "        self.train_critic_2 = Critic().to(self.device)\n",
    "        self.target_critic_2 = Critic().to(self.device).eval()\n",
    "        self.hard_update(self.train_critic_2, self.target_critic_2) # hard update at the beginning\n",
    "        self.critic_2_optim = torch.optim.AdamW(self.train_critic_2.parameters(), lr=lr, weight_decay=weight_decay, amsgrad=True)\n",
    "        print(f'Number of paramters of Single Critic Net: {sum(p.numel() for p in self.train_critic_2.parameters())}')\n",
    "        \n",
    "        self.memory= ReplayBuffer(action_size= action_size, buffer_size= buffer_size, \\\n",
    "            batch_size= self.batch_size, device=self.device)\n",
    "\n",
    "        self.mse_loss = torch.nn.MSELoss()\n",
    "        \n",
    "    def learn_with_batches(self, state, action, reward, next_state, done):\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        self.learn_one_step()\n",
    "\n",
    "    def learn_one_step(self):\n",
    "        if(len(self.memory)>self.batch_size):\n",
    "            exp=self.memory.sample()\n",
    "            self.learn(exp)        \n",
    "            \n",
    "    def learn(self, exp):\n",
    "        self.learn_call+=1\n",
    "        states, actions, rewards, next_states, done = exp\n",
    "        \n",
    "        #update critic\n",
    "        with torch.no_grad():\n",
    "            next_actions, next_entropies = self.train_actor(next_states)\n",
    "            Q_targets_next_1 = self.target_critic_1(next_states, next_actions)\n",
    "            Q_targets_next_2 = self.target_critic_2(next_states, next_actions)\n",
    "            Q_targets_next = torch.min(Q_targets_next_1, Q_targets_next_2) + self.alpha * next_entropies\n",
    "            Q_targets = rewards + (self.gamma * Q_targets_next * (1-done))\n",
    "            #Q_targets = rewards + (self.gamma * Q_targets_next)\n",
    "\n",
    "        Q_expected_1 = self.train_critic_1(states, actions)\n",
    "        critic_1_loss = self.mse_loss(Q_expected_1, Q_targets)\n",
    "        #critic_1_loss = torch.nn.SmoothL1Loss()(Q_expected_1, Q_targets)\n",
    "        \n",
    "        self.critic_1_optim.zero_grad(set_to_none=True)\n",
    "        critic_1_loss.backward()\n",
    "        #torch.nn.utils.clip_grad_norm_(self.train_critic_1.parameters(), 1)\n",
    "        self.critic_1_optim.step()\n",
    "\n",
    "        Q_expected_2 = self.train_critic_2(states, actions)   \n",
    "        critic_2_loss = self.mse_loss(Q_expected_2, Q_targets)\n",
    "        #critic_2_loss = torch.nn.SmoothL1Loss()(Q_expected_2, Q_targets)\n",
    "        \n",
    "        self.critic_2_optim.zero_grad(set_to_none=True)\n",
    "        critic_2_loss.backward()\n",
    "        #torch.nn.utils.clip_grad_norm_(self.train_critic_2.parameters(), 1)\n",
    "        self.critic_2_optim.step()\n",
    "\n",
    "        #update actor\n",
    "        actions_pred, entropies_pred = self.train_actor(states)\n",
    "        Q_pi = torch.min(self.train_critic_1(states, actions_pred), self.train_critic_2(states, actions_pred))\n",
    "        actor_loss = -(Q_pi + self.alpha * entropies_pred).mean()\n",
    "        \n",
    "        self.actor_optim.zero_grad(set_to_none=True)\n",
    "        actor_loss.backward()\n",
    "        #torch.nn.utils.clip_grad_norm_(self.train_actor.parameters(), 1)\n",
    "        self.actor_optim.step()\n",
    "\n",
    "        if self.learn_call % self.update_freq == 0:\n",
    "            self.learn_call = 0        \n",
    "            #using soft upates\n",
    "            self.soft_update(self.train_critic_1, self.target_critic_1)\n",
    "            self.soft_update(self.train_critic_2, self.target_critic_2)\n",
    "\n",
    "    @torch.no_grad()        \n",
    "    def get_action(self, state, explore=True):\n",
    "        #self.train_actor.eval()\n",
    "        state = torch.from_numpy(state).unsqueeze(0).float().to(self.device)\n",
    "        #with torch.no_grad():\n",
    "        action, entropy = self.train_actor(state, explore=explore)\n",
    "        action = action.cpu().data.numpy()[0]\n",
    "        #self.train_actor.train()\n",
    "        return action\n",
    "    \n",
    "    def soft_update(self, local_model, target_model):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(self.tau*local_param.data + (1.0-self.tau)*target_param.data)\n",
    "\n",
    "    def hard_update(self, local_model, target_model):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(local_param.data)\n",
    "             \n",
    "\n",
    "\n",
    "    def save_ckpt(self,save_path=\"Models/sac/sac_bipedal_4.pt\"):\n",
    "        save_path = save_path\n",
    "        # Create a dictionary containing the state_dict of the actor, critic_1, and critic_2\n",
    "        checkpoint = {\n",
    "            'actor': self.train_actor.state_dict(),\n",
    "            'critic_1': self.train_critic_1.state_dict(),\n",
    "            'critic_2': self.train_critic_2.state_dict()\n",
    "        }\n",
    "        \n",
    "        # Save the dictionary to a single file\n",
    "        torch.save(checkpoint, save_path)\n",
    "\n",
    "    def load_ckpt(self,load_path=\"Models/sac/sac_bipedal_4.pt\"):\n",
    "        load_path = load_path\n",
    "        \n",
    "        # Load the checkpoint dictionary\n",
    "        try:\n",
    "            checkpoint = torch.load(load_path)\n",
    "            \n",
    "            # Load the state_dicts into the models\n",
    "            self.train_actor.load_state_dict(checkpoint['actor'])\n",
    "            self.train_critic_1.load_state_dict(checkpoint['critic_1'])\n",
    "            self.train_critic_2.load_state_dict(checkpoint['critic_2'])\n",
    "            print(f\"Model loaded successfully: {load_path}\")\n",
    "        except:\n",
    "            print(\"Failed to load\")\n",
    "\n",
    "\n",
    "\n",
    "    def train_bipedal_walker(self, env, num_episodes=500, max_timesteps=2000, render=False):\n",
    "        \"\"\"\n",
    "        Trains a Soft Actor-Critic agent on the Bipedal Walker environment.\n",
    "\n",
    "        :param agent: SACAgent object\n",
    "        :param env: Bipedal Walker environment (gym.make('BipedalWalker-v3'))\n",
    "        :param num_episodes: Number of episodes to train for\n",
    "        :param max_timesteps: Maximum number of timesteps per episode\n",
    "        :param render: Whether to render the environment or not\n",
    "        \"\"\"\n",
    "        rewards_history = []\n",
    "        for episode in range(1, num_episodes + 1):\n",
    "            state = env.reset()\n",
    "            episode_reward = 0\n",
    "\n",
    "            for t in range(max_timesteps):\n",
    "                if render:\n",
    "                    env.render()\n",
    "\n",
    "                # Get action from the agent\n",
    "                action = self.get_action(state, explore=True)\n",
    "\n",
    "                # Take the action in the environment\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                # Clip reward to avoid large fluctuations\n",
    "                reward = np.clip(reward, -10, 10)\n",
    "\n",
    "                # Store the experience in replay buffer\n",
    "                self.learn_with_batches(state, action, reward, next_state, done)\n",
    "\n",
    "                # Move to the next state\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            rewards_history.append(episode_reward)\n",
    "            avg_reward = np.mean(rewards_history[-100:])\n",
    "\n",
    "            print(f\"Episode {episode}/{num_episodes}, Reward: {episode_reward:.2f}, Avg Reward (Last 100): {avg_reward:.2f}\")\n",
    "\n",
    "            if episode % 5 == 0:\n",
    "                self.save_ckpt()\n",
    "\n",
    "        env.close()\n",
    "        return rewards_history\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "import random\n",
    "from torch.distributions import Normal\n",
    "\n",
    "EPS = 0.003\n",
    "\n",
    "class FeedForwardEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, ff_size):\n",
    "        super(FeedForwardEncoder, self).__init__()\n",
    "        self.embedding = nn.Linear(input_size, hidden_size)\n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "        nn.init.zeros_(self.embedding.bias)\n",
    "        self.block = nn.Sequential(nn.LayerNorm(hidden_size), nn.Linear(hidden_size, ff_size), nn.GELU(), nn.Linear(ff_size, hidden_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x + self.block(x)\n",
    "        return x\n",
    "\n",
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self, state_dim=24, action_dim=4):\n",
    "        \"\"\"\n",
    "        :param state_dim: Dimension of input state (int)\n",
    "        :param action_dim: Dimension of input action (int)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        self.state_encoder = FeedForwardEncoder(self.state_dim, 96, 192)\n",
    "\n",
    "        self.fc2 = nn.Linear(96 + self.action_dim, 192)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight, gain=nn.init.calculate_gain('tanh'))\n",
    "        \n",
    "        self.fc_out = nn.Linear(192, 1, bias=False)\n",
    "        nn.init.uniform_(self.fc_out.weight, -0.003,+0.003)\n",
    "\n",
    "        self.act = nn.Tanh()\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"\n",
    "        returns Value function Q(s,a) obtained from critic network\n",
    "        :param state: Input state (Torch Variable : [n,state_dim] )\n",
    "        :param action: Input Action (Torch Variable : [n,action_dim] )\n",
    "        :return: Value function : Q(S,a) (Torch Variable : [n,1] )\n",
    "        \"\"\"\n",
    "        s = self.state_encoder(state)\n",
    "        x = torch.cat((s,action),dim=1)\n",
    "        x = self.act(self.fc2(x))\n",
    "        x = self.fc_out(x)*10\n",
    "        return x\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "\n",
    "    def __init__(self, state_dim=24, action_dim=4, stochastic=False):\n",
    "        \"\"\"\n",
    "        :param state_dim: Dimension of input state (int)\n",
    "        :param action_dim: Dimension of output action (int)\n",
    "        :param action_lim: Used to limit action in [-action_lim,action_lim]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.stochastic = stochastic\n",
    "\n",
    "        self.state_encoder = FeedForwardEncoder(self.state_dim, 96, 192)\n",
    "\n",
    "        self.fc = nn.Linear(96, action_dim, bias=False)\n",
    "        nn.init.uniform_(self.fc.weight, -0.003,+0.003)\n",
    "        #nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "        if self.stochastic:\n",
    "            self.log_std = nn.Linear(96, action_dim, bias=False)\n",
    "            nn.init.uniform_(self.log_std.weight, -0.003,+0.003)\n",
    "            #nn.init.zeros_(self.log_std.bias)   \n",
    "\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "\n",
    "    def forward(self, state, explore=True):\n",
    "        \"\"\"\n",
    "        returns either:\n",
    "        - deterministic policy function mu(s) as policy action.\n",
    "        - stochastic action sampled from tanh-gaussian policy, with its entropy value.\n",
    "        this function returns actions lying in (-1,1) \n",
    "        :param state: Input state (Torch Variable : [n,state_dim] )\n",
    "        :return: Output action (Torch Variable: [n,action_dim] )\n",
    "        \"\"\"\n",
    "        s = self.state_encoder(state)\n",
    "        if self.stochastic:\n",
    "            means = self.fc(s)\n",
    "            log_stds = self.log_std(s)\n",
    "            log_stds = torch.clamp(log_stds, min=-10.0, max=2.0)\n",
    "            stds = log_stds.exp()\n",
    "            #print(stds)\n",
    "            dists = Normal(means, stds)\n",
    "            if explore:\n",
    "                x = dists.rsample()\n",
    "            else:\n",
    "                x = means\n",
    "            actions = self.tanh(x)\n",
    "            log_probs = dists.log_prob(x) - torch.log(1-actions.pow(2) + 1e-6)\n",
    "            entropies = -log_probs.sum(dim=1, keepdim=True)\n",
    "            return actions, entropies\n",
    "\n",
    "        else:\n",
    "            actions = self.tanh(self.fc(s))\n",
    "            return actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of paramters of Actor Net: 40512\n",
      "Number of paramters of Single Critic Net: 59328\n",
      "Model loaded successfully: Models/sac/sac_bipedal.pt\n"
     ]
    }
   ],
   "source": [
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.shape[0]\n",
    "agent = SACAgent(Actor, Critic, clip_low=env.action_space.low, clip_high=env.action_space.high, state_size=state_size, action_size=action_size)\n",
    "agent.load_ckpt(load_path=\"Models/sac/sac_bipedal.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/10, Reward: 1.10, Avg Reward (Last 100): 1.10\n",
      "Episode 2/10, Reward: 0.10, Avg Reward (Last 100): 0.60\n",
      "Episode 3/10, Reward: -0.38, Avg Reward (Last 100): 0.28\n",
      "Episode 4/10, Reward: 120.54, Avg Reward (Last 100): 30.34\n",
      "Episode 5/10, Reward: 6.80, Avg Reward (Last 100): 25.63\n",
      "Episode 6/10, Reward: 70.91, Avg Reward (Last 100): 33.18\n",
      "Episode 7/10, Reward: -7.88, Avg Reward (Last 100): 27.31\n",
      "Episode 8/10, Reward: 36.18, Avg Reward (Last 100): 28.42\n",
      "Episode 9/10, Reward: -3.69, Avg Reward (Last 100): 24.85\n",
      "Episode 10/10, Reward: 26.41, Avg Reward (Last 100): 25.01\n"
     ]
    }
   ],
   "source": [
    "rewards_history = agent.train_bipedal_walker(env, num_episodes=10, max_timesteps=1600, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_bipedal_walker(agent, env, num_episodes=5, max_timesteps=2000, render=True):\n",
    "    \"\"\"\n",
    "    Trains a Soft Actor-Critic agent on the Bipedal Walker environment.\n",
    "    \n",
    "    :param agent: SACAgent object\n",
    "    :param env: Bipedal Walker environment (gym.make('BipedalWalker-v3'))\n",
    "    :param num_episodes: Number of episodes to train for\n",
    "    :param max_timesteps: Maximum number of timesteps per episode\n",
    "    :param render: Whether to render the environment or not\n",
    "    \"\"\"\n",
    "    rewards_history = []\n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        state = env.reset()\n",
    "        \n",
    "        episode_reward = 0\n",
    "\n",
    "        for t in range(max_timesteps):\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            # Get action from the agent\n",
    "            action = agent.get_action(state, explore=True)\n",
    "            \n",
    "            # Take the action in the environment\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "              # Move to the next state\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        rewards_history.append(episode_reward)\n",
    "        avg_reward = np.mean(rewards_history[-100:])\n",
    "\n",
    "        print(f\"Episode {episode}/{num_episodes}, Reward: {episode_reward:.2f}, Avg Reward (Last 100): {avg_reward:.2f}\")\n",
    "\n",
    "       \n",
    "\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/5, Reward: 2.60, Avg Reward (Last 100): 2.60\n",
      "Episode 2/5, Reward: 12.97, Avg Reward (Last 100): 7.79\n",
      "Episode 3/5, Reward: 45.67, Avg Reward (Last 100): 20.41\n",
      "Episode 4/5, Reward: 2.16, Avg Reward (Last 100): 15.85\n",
      "Episode 5/5, Reward: 280.30, Avg Reward (Last 100): 68.74\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('BipedalWalker-v3', hardcore=True,render_mode='human')\n",
    "env = MyWalkerWrapper(env, skip=2)\n",
    "test_bipedal_walker(agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
