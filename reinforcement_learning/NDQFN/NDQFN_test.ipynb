{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantileNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size, num_quantiles):\n",
    "        super(QuantileNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, action_size * num_quantiles)\n",
    "        self.action_size = action_size\n",
    "        self.num_quantiles = num_quantiles\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        quantiles = x.view(-1, self.action_size, self.num_quantiles)\n",
    "        return quantiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, state_size, action_size, num_quantiles, gamma, lr, batch_size, buffer_size, device):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.num_quantiles = num_quantiles\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.device = device\n",
    "\n",
    "        self.q_network = QuantileNetwork(state_size, action_size, num_quantiles).to(self.device)\n",
    "        self.target_network = QuantileNetwork(state_size, action_size, num_quantiles).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        \n",
    "        self.update_target_network()\n",
    "        \n",
    "        # Initialize taus\n",
    "        self.taus = torch.arange(0.5 / self.num_quantiles, 1.0, 1.0 / self.num_quantiles).to(self.device)\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "    def act(self, state, epsilon=0.1):\n",
    "        if random.random() > epsilon:\n",
    "            state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                quantiles = self.q_network(state)\n",
    "                q_values = quantiles.mean(dim=2)\n",
    "                action = q_values.max(1)[1].item()\n",
    "        else:\n",
    "            action = random.choice(np.arange(self.action_size))\n",
    "        return action\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            self.learn()\n",
    "\n",
    "    def learn(self):\n",
    "        states, actions, rewards, next_states, dones = zip(*random.sample(self.memory, self.batch_size))\n",
    "        \n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.LongTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)\n",
    "\n",
    "        # Get target quantiles\n",
    "        with torch.no_grad():\n",
    "            next_quantiles = self.target_network(next_states)\n",
    "            next_q_values = next_quantiles.mean(dim=2)\n",
    "            next_actions = next_q_values.max(1)[1].unsqueeze(1).unsqueeze(1).expand(self.batch_size, 1, self.num_quantiles)\n",
    "            next_best_quantiles = next_quantiles.gather(1, next_actions).squeeze(1)\n",
    "            target_quantiles = rewards.unsqueeze(1) + (self.gamma * next_best_quantiles * (1 - dones.unsqueeze(1)))\n",
    "\n",
    "        # Get current quantiles\n",
    "        quantiles = self.q_network(states)\n",
    "        actions = actions.unsqueeze(1).unsqueeze(1).expand(self.batch_size, 1, self.num_quantiles)\n",
    "        current_quantiles = quantiles.gather(1, actions).squeeze(1)\n",
    "\n",
    "        # Ensure non-decreasing quantiles\n",
    "        current_quantiles = torch.sort(current_quantiles, dim=1)[0]\n",
    "\n",
    "        # Compute quantile regression loss\n",
    "        td_errors = target_quantiles.unsqueeze(2) - current_quantiles.unsqueeze(1)\n",
    "        huber_loss = 0.5 * td_errors.pow(2) * (td_errors.abs() <= 1.0).float() + (td_errors.abs() - 0.5) * (td_errors.abs() > 1.0).float()\n",
    "        quantile_loss = torch.abs(self.taus.unsqueeze(0).unsqueeze(0) - (td_errors < 0).float()) * huber_loss\n",
    "        loss = quantile_loss.sum(dim=1).mean(dim=1).mean()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def save(self, filename):\n",
    "        torch.save({\n",
    "            'q_network_state_dict': self.q_network.state_dict(),\n",
    "            'target_network_state_dict': self.target_network.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "        }, filename)\n",
    "\n",
    "    def load(self, filename):\n",
    "        checkpoint = torch.load(filename, map_location=self.device)\n",
    "        self.q_network.load_state_dict(checkpoint['q_network_state_dict'])\n",
    "        self.target_network.load_state_dict(checkpoint['target_network_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.q_network.to(self.device)\n",
    "        self.target_network.to(self.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(num_episodes,load_path=None, device='cpu'):\n",
    "    env = gym.make('LunarLander-v2',render_mode= 'human')\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    num_quantiles = 51\n",
    "    gamma = 0.99\n",
    "    lr = 0.001\n",
    "    batch_size = 64\n",
    "    buffer_size = 100000\n",
    "    device = torch.device(device)\n",
    "\n",
    "    agent = Agent(state_size, action_size, num_quantiles, gamma, lr, batch_size, buffer_size, device)\n",
    "    \n",
    "    if load_path:\n",
    "        agent.load(load_path)\n",
    "        print(f\"Loaded model from {load_path}\")\n",
    "\n",
    "    for e in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "        \n",
    "        print(f\"Episode {e+1}/{num_episodes}, Total Reward: {total_reward}\")\n",
    "    env.close()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from nqfn_lunar_lander.pth\n",
      "Episode 1/10, Total Reward: 278.18287873173983\n",
      "Episode 2/10, Total Reward: 231.53907621572648\n",
      "Episode 3/10, Total Reward: 275.96262742313814\n",
      "Episode 4/10, Total Reward: 256.4528824094258\n",
      "Episode 5/10, Total Reward: 285.74298931104886\n",
      "Episode 6/10, Total Reward: 260.1863867506504\n",
      "Episode 7/10, Total Reward: 276.49075048501606\n",
      "Episode 8/10, Total Reward: 257.05529109964937\n",
      "Episode 9/10, Total Reward: 266.20049155292406\n",
      "Episode 10/10, Total Reward: 247.0169547618857\n"
     ]
    }
   ],
   "source": [
    "test_agent(10,load_path='nqfn_lunar_lander.pth', device='cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
