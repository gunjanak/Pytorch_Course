{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janak/Documents/Pytorch_CPU/venv/lib/python3.10/site-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/home/janak/Documents/Pytorch_CPU/venv/lib/python3.10/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "env = gym.envs.make(\"LunarLander-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experience tuple for replay buffer\n",
    "Experience = namedtuple('Experience', ('state', 'action', 'reward', 'next_state', 'done'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, num_quantiles, hidden_dim=128):\n",
    "        super(IQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, action_dim * num_quantiles)\n",
    "        self.num_quantiles = num_quantiles\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "    def forward(self, x, taus):\n",
    "        batch_size = x.size(0)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        quantiles = self.fc3(x).view(batch_size, self.num_quantiles, self.action_dim)\n",
    "        return quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def add_experience(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        return Experience(*zip(*batch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_quantiles = 10\n",
    "hidden_dim = 128\n",
    "capacity = 10000\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "update_freq = 10  # Update target network every 10 episodes or steps\n",
    "num_episodes = 1000  # Number of training episodes\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.01\n",
    "epsilon_decay = 0.995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize networks and optimizer\n",
    "main_net = IQN(state_dim, action_dim, num_quantiles, hidden_dim).to(device)\n",
    "target_net = IQN(state_dim, action_dim, num_quantiles, hidden_dim).to(device)\n",
    "# target_net.load_state_dict(main_net.state_dict())  # Initialize target network with main network's parameters\n",
    "optimizer = optim.Adam(main_net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename='checkpoint.pth'):\n",
    "    torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(filename='checkpoint.pth', map_location=None):\n",
    "    if map_location:\n",
    "        return torch.load(filename, map_location=map_location)\n",
    "    return torch.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from episode 251\n"
     ]
    }
   ],
   "source": [
    "# Load model if available\n",
    "checkpoint_path = 'IQN_lunar_lander.pth'\n",
    "try:\n",
    "    checkpoint = load_checkpoint(checkpoint_path)\n",
    "    main_net.load_state_dict(checkpoint['main_net_state_dict'])\n",
    "    target_net.load_state_dict(checkpoint['target_net_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epsilon = checkpoint['epsilon']\n",
    "    start_episode = checkpoint['episode'] + 1\n",
    "    print(f\"Loaded checkpoint from episode {start_episode}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No checkpoint found, starting from scratch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_net.load_state_dict(main_net.state_dict())  # Initialize target network with main network's parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize replay buffer\n",
    "replay_buffer = ReplayBuffer(capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_huber_loss(predictions, targets, taus, kappa=1.0):\n",
    "    \"\"\"Calculates the quantile Huber loss.\"\"\"\n",
    "    u = targets - predictions\n",
    "    abs_u = torch.abs(u)\n",
    "    huber_loss = torch.where(abs_u <= kappa, 0.5 * u ** 2, kappa * (abs_u - 0.5 * kappa))\n",
    "    loss = (torch.abs(taus - (u < 0).float()) * huber_loss).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "episode_rewards = []\n",
    "epsilon = epsilon_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Reward: -93.33779579527578, Epsilon: 0.995\n",
      "Checkpoint saved at episode 0\n",
      "Episode 1, Reward: -41.0730593065657, Epsilon: 0.990025\n",
      "Episode 2, Reward: -100.04860265017787, Epsilon: 0.985074875\n",
      "Episode 3, Reward: -288.04664662710957, Epsilon: 0.9801495006250001\n",
      "Episode 4, Reward: -167.40142037074318, Epsilon: 0.9752487531218751\n",
      "Episode 5, Reward: -136.78710516066715, Epsilon: 0.9703725093562657\n",
      "Episode 6, Reward: -377.3619240888442, Epsilon: 0.9655206468094844\n",
      "Episode 7, Reward: -117.5860409270703, Epsilon: 0.960693043575437\n",
      "Episode 8, Reward: -97.64211198198075, Epsilon: 0.9558895783575597\n",
      "Episode 9, Reward: -152.21976949439943, Epsilon: 0.9511101304657719\n",
      "Episode 10, Reward: -133.5577737952258, Epsilon: 0.946354579813443\n",
      "Episode 11, Reward: -101.11919574765388, Epsilon: 0.9416228069143757\n",
      "Episode 12, Reward: -177.5118428332503, Epsilon: 0.9369146928798039\n",
      "Episode 13, Reward: -69.63324316770924, Epsilon: 0.9322301194154049\n",
      "Episode 14, Reward: -88.7137438132228, Epsilon: 0.9275689688183278\n",
      "Episode 15, Reward: -118.80045524891598, Epsilon: 0.9229311239742362\n",
      "Episode 16, Reward: -107.65441745740735, Epsilon: 0.918316468354365\n",
      "Episode 17, Reward: -73.09708881950124, Epsilon: 0.9137248860125932\n",
      "Episode 18, Reward: -87.08460894041934, Epsilon: 0.9091562615825302\n",
      "Episode 19, Reward: -181.15746594062358, Epsilon: 0.9046104802746175\n",
      "Episode 20, Reward: -121.83503594412073, Epsilon: 0.9000874278732445\n",
      "Episode 21, Reward: -112.94348569368478, Epsilon: 0.8955869907338783\n",
      "Episode 22, Reward: -105.15744539776794, Epsilon: 0.8911090557802088\n",
      "Episode 23, Reward: -101.24016759994933, Epsilon: 0.8866535105013078\n",
      "Episode 24, Reward: -106.20152379337873, Epsilon: 0.8822202429488013\n",
      "Episode 25, Reward: -47.16041156676329, Epsilon: 0.8778091417340573\n",
      "Episode 26, Reward: -171.0544411404852, Epsilon: 0.8734200960253871\n",
      "Episode 27, Reward: -103.26528241723057, Epsilon: 0.8690529955452602\n",
      "Episode 28, Reward: -98.33886539067677, Epsilon: 0.8647077305675338\n",
      "Episode 29, Reward: -49.552795633340864, Epsilon: 0.8603841919146962\n",
      "Episode 30, Reward: -240.23070301659698, Epsilon: 0.8560822709551227\n",
      "Episode 31, Reward: -111.5398822769948, Epsilon: 0.851801859600347\n",
      "Episode 32, Reward: -67.41643453301089, Epsilon: 0.8475428503023453\n",
      "Episode 33, Reward: -114.57584095153044, Epsilon: 0.8433051360508336\n",
      "Episode 34, Reward: -138.0496920716013, Epsilon: 0.8390886103705794\n",
      "Episode 35, Reward: -72.7918541024776, Epsilon: 0.8348931673187264\n",
      "Episode 36, Reward: -72.65039413911519, Epsilon: 0.8307187014821328\n",
      "Episode 37, Reward: -95.3562794132158, Epsilon: 0.8265651079747222\n",
      "Episode 38, Reward: -62.819232600007496, Epsilon: 0.8224322824348486\n",
      "Episode 39, Reward: -27.429851985845744, Epsilon: 0.8183201210226743\n",
      "Episode 40, Reward: -54.7744763840037, Epsilon: 0.8142285204175609\n",
      "Episode 41, Reward: -106.17545974684865, Epsilon: 0.810157377815473\n",
      "Episode 42, Reward: -78.47633707058189, Epsilon: 0.8061065909263957\n",
      "Episode 43, Reward: -111.93426205399952, Epsilon: 0.8020760579717637\n",
      "Episode 44, Reward: -88.36700127354484, Epsilon: 0.798065677681905\n",
      "Episode 45, Reward: -74.22723305009558, Epsilon: 0.7940753492934954\n",
      "Episode 46, Reward: -78.65674688357586, Epsilon: 0.7901049725470279\n",
      "Episode 47, Reward: -75.96152367807883, Epsilon: 0.7861544476842928\n",
      "Episode 48, Reward: -89.92351094356215, Epsilon: 0.7822236754458713\n",
      "Episode 49, Reward: -60.4738118200931, Epsilon: 0.778312557068642\n",
      "Episode 50, Reward: -125.79446447299566, Epsilon: 0.7744209942832988\n",
      "Checkpoint saved at episode 50\n",
      "Episode 51, Reward: -115.0990144745823, Epsilon: 0.7705488893118823\n",
      "Episode 52, Reward: -99.73238582304305, Epsilon: 0.7666961448653229\n",
      "Episode 53, Reward: -77.7364571484429, Epsilon: 0.7628626641409962\n",
      "Episode 54, Reward: -119.3978950476337, Epsilon: 0.7590483508202912\n",
      "Episode 55, Reward: -77.87580450268254, Epsilon: 0.7552531090661897\n",
      "Episode 56, Reward: -81.05884891893963, Epsilon: 0.7514768435208588\n",
      "Episode 57, Reward: -71.43656621153403, Epsilon: 0.7477194593032545\n",
      "Episode 58, Reward: -85.17947805701654, Epsilon: 0.7439808620067382\n",
      "Episode 59, Reward: -16.088749582864438, Epsilon: 0.7402609576967045\n",
      "Episode 60, Reward: -115.48820784218148, Epsilon: 0.736559652908221\n",
      "Episode 61, Reward: -84.77693078119854, Epsilon: 0.7328768546436799\n",
      "Episode 62, Reward: -45.979101370969886, Epsilon: 0.7292124703704616\n",
      "Episode 63, Reward: -46.79074516846807, Epsilon: 0.7255664080186093\n",
      "Episode 64, Reward: -60.34478931790477, Epsilon: 0.7219385759785162\n",
      "Episode 65, Reward: -60.61431380597489, Epsilon: 0.7183288830986236\n",
      "Episode 66, Reward: -48.38997050748094, Epsilon: 0.7147372386831305\n",
      "Episode 67, Reward: -77.87855603778088, Epsilon: 0.7111635524897149\n",
      "Episode 68, Reward: -116.90519355766558, Epsilon: 0.7076077347272662\n",
      "Episode 69, Reward: -74.23667331670177, Epsilon: 0.7040696960536299\n",
      "Episode 70, Reward: -71.9023212323537, Epsilon: 0.7005493475733617\n",
      "Episode 71, Reward: -95.51715917982143, Epsilon: 0.697046600835495\n",
      "Episode 72, Reward: -77.50649237604722, Epsilon: 0.6935613678313175\n",
      "Episode 73, Reward: -137.3897984273048, Epsilon: 0.6900935609921609\n",
      "Episode 74, Reward: -31.747628868218825, Epsilon: 0.6866430931872001\n",
      "Episode 75, Reward: -113.87464939037713, Epsilon: 0.6832098777212641\n",
      "Episode 76, Reward: -79.15492925991497, Epsilon: 0.6797938283326578\n",
      "Episode 77, Reward: -43.85563612611402, Epsilon: 0.6763948591909945\n",
      "Episode 78, Reward: -40.50122999731475, Epsilon: 0.6730128848950395\n",
      "Episode 79, Reward: 2.3290240563423055, Epsilon: 0.6696478204705644\n",
      "Episode 80, Reward: -53.24710164817647, Epsilon: 0.6662995813682115\n",
      "Episode 81, Reward: -26.34416168167239, Epsilon: 0.6629680834613705\n",
      "Episode 82, Reward: -29.336981835216577, Epsilon: 0.6596532430440636\n",
      "Episode 83, Reward: -73.37921970683712, Epsilon: 0.6563549768288433\n",
      "Episode 84, Reward: -62.936620849112956, Epsilon: 0.653073201944699\n",
      "Episode 85, Reward: -100.59575052423425, Epsilon: 0.6498078359349755\n",
      "Episode 86, Reward: -42.21791135550433, Epsilon: 0.6465587967553006\n",
      "Episode 87, Reward: -67.45054232486119, Epsilon: 0.6433260027715241\n",
      "Episode 88, Reward: -11.030270556788366, Epsilon: 0.6401093727576664\n",
      "Episode 89, Reward: -77.79643995857019, Epsilon: 0.6369088258938781\n",
      "Episode 90, Reward: -19.491066619753383, Epsilon: 0.6337242817644086\n",
      "Episode 91, Reward: -74.6560343591829, Epsilon: 0.6305556603555866\n",
      "Episode 92, Reward: -53.57308741771732, Epsilon: 0.6274028820538087\n",
      "Episode 93, Reward: -39.44689993430319, Epsilon: 0.6242658676435396\n",
      "Episode 94, Reward: -33.26216028636529, Epsilon: 0.6211445383053219\n",
      "Episode 95, Reward: 1.782915458513287, Epsilon: 0.6180388156137953\n",
      "Episode 96, Reward: -80.80250144278533, Epsilon: 0.6149486215357263\n",
      "Episode 97, Reward: -58.40104571343642, Epsilon: 0.6118738784280476\n",
      "Episode 98, Reward: -27.01893179779057, Epsilon: 0.6088145090359074\n",
      "Episode 99, Reward: -57.09472467217482, Epsilon: 0.6057704364907278\n",
      "Episode 100, Reward: -57.28380106568288, Epsilon: 0.6027415843082742\n",
      "Checkpoint saved at episode 100\n",
      "Episode 101, Reward: -257.322225832257, Epsilon: 0.5997278763867329\n",
      "Episode 102, Reward: -28.11357309788353, Epsilon: 0.5967292370047992\n",
      "Episode 103, Reward: -9.046237830327868, Epsilon: 0.5937455908197752\n",
      "Episode 104, Reward: -54.67968182247655, Epsilon: 0.5907768628656763\n",
      "Episode 105, Reward: -18.307827006887365, Epsilon: 0.5878229785513479\n",
      "Episode 106, Reward: -28.92546137773786, Epsilon: 0.5848838636585911\n",
      "Episode 107, Reward: 25.880950788697845, Epsilon: 0.5819594443402982\n",
      "Episode 108, Reward: -75.02224016994299, Epsilon: 0.5790496471185967\n",
      "Episode 109, Reward: -22.952260171528508, Epsilon: 0.5761543988830038\n",
      "Episode 110, Reward: -65.29261173942957, Epsilon: 0.5732736268885887\n",
      "Episode 111, Reward: -23.192836213649883, Epsilon: 0.5704072587541458\n",
      "Episode 112, Reward: -76.03936020976911, Epsilon: 0.567555222460375\n",
      "Episode 113, Reward: -49.28362730008063, Epsilon: 0.5647174463480732\n",
      "Episode 114, Reward: -27.191527596886786, Epsilon: 0.5618938591163328\n",
      "Episode 115, Reward: -32.61586716279709, Epsilon: 0.5590843898207511\n",
      "Episode 116, Reward: -48.63899619696495, Epsilon: 0.5562889678716474\n",
      "Episode 117, Reward: -50.49396513651969, Epsilon: 0.5535075230322891\n",
      "Episode 118, Reward: -13.237065599813022, Epsilon: 0.5507399854171277\n",
      "Episode 119, Reward: -82.55787302165217, Epsilon: 0.547986285490042\n",
      "Episode 120, Reward: -25.074583705458664, Epsilon: 0.5452463540625918\n",
      "Episode 121, Reward: 5.580534590576946, Epsilon: 0.5425201222922789\n",
      "Episode 122, Reward: -23.33420925630749, Epsilon: 0.5398075216808175\n",
      "Episode 123, Reward: -18.996265446655897, Epsilon: 0.5371084840724134\n",
      "Episode 124, Reward: 24.121079202455363, Epsilon: 0.5344229416520513\n",
      "Episode 125, Reward: -79.92904412474618, Epsilon: 0.531750826943791\n",
      "Episode 126, Reward: -30.262502134475483, Epsilon: 0.5290920728090721\n",
      "Episode 127, Reward: -76.79176846811586, Epsilon: 0.5264466124450268\n",
      "Episode 128, Reward: 60.65520931597774, Epsilon: 0.5238143793828016\n",
      "Episode 129, Reward: 22.653151923215844, Epsilon: 0.5211953074858876\n",
      "Episode 130, Reward: -32.647501495051344, Epsilon: 0.5185893309484582\n",
      "Episode 131, Reward: -86.93762385903825, Epsilon: 0.5159963842937159\n",
      "Episode 132, Reward: -43.95610798336419, Epsilon: 0.5134164023722473\n",
      "Episode 133, Reward: -31.994444940663712, Epsilon: 0.510849320360386\n",
      "Episode 134, Reward: 57.74422071174229, Epsilon: 0.5082950737585841\n",
      "Episode 135, Reward: -45.182115553919004, Epsilon: 0.5057535983897912\n",
      "Episode 136, Reward: -7.318408979906948, Epsilon: 0.5032248303978422\n",
      "Episode 137, Reward: -52.06575453512277, Epsilon: 0.500708706245853\n",
      "Episode 138, Reward: 11.242750656248063, Epsilon: 0.4982051627146237\n",
      "Episode 139, Reward: 2.5431046978965526, Epsilon: 0.49571413690105054\n",
      "Episode 140, Reward: -57.040353828090474, Epsilon: 0.4932355662165453\n",
      "Episode 141, Reward: -35.10077996663355, Epsilon: 0.4907693883854626\n",
      "Episode 142, Reward: -13.967716530672917, Epsilon: 0.4883155414435353\n",
      "Episode 143, Reward: -69.85559978100596, Epsilon: 0.4858739637363176\n",
      "Episode 144, Reward: -43.07190262266809, Epsilon: 0.483444593917636\n",
      "Episode 145, Reward: -41.32555523448417, Epsilon: 0.4810273709480478\n",
      "Episode 146, Reward: -29.877391300324703, Epsilon: 0.47862223409330756\n",
      "Episode 147, Reward: -51.41648976372451, Epsilon: 0.47622912292284103\n",
      "Episode 148, Reward: -19.292025842040275, Epsilon: 0.4738479773082268\n",
      "Episode 149, Reward: -15.991649887572208, Epsilon: 0.47147873742168567\n",
      "Episode 150, Reward: -76.29383447806401, Epsilon: 0.46912134373457726\n",
      "Checkpoint saved at episode 150\n",
      "Episode 151, Reward: -44.9830140339329, Epsilon: 0.46677573701590436\n",
      "Episode 152, Reward: -155.2395492137722, Epsilon: 0.46444185833082485\n",
      "Episode 153, Reward: -16.665765404787578, Epsilon: 0.46211964903917074\n",
      "Episode 154, Reward: 6.6506888156499855, Epsilon: 0.4598090507939749\n",
      "Episode 155, Reward: -43.4948370762437, Epsilon: 0.457510005540005\n",
      "Episode 156, Reward: 18.070429583766057, Epsilon: 0.45522245551230495\n",
      "Episode 157, Reward: 28.28507988699758, Epsilon: 0.4529463432347434\n",
      "Episode 158, Reward: -160.0092808842893, Epsilon: 0.4506816115185697\n",
      "Episode 159, Reward: -20.211117767397, Epsilon: 0.4484282034609769\n",
      "Episode 160, Reward: 22.516520723494224, Epsilon: 0.446186062443672\n",
      "Episode 161, Reward: -48.57006953537147, Epsilon: 0.4439551321314536\n",
      "Episode 162, Reward: -104.28954734151037, Epsilon: 0.4417353564707963\n",
      "Episode 163, Reward: -65.13860094601, Epsilon: 0.43952667968844233\n",
      "Episode 164, Reward: -15.269436633531384, Epsilon: 0.43732904629000013\n",
      "Episode 165, Reward: -32.42345575018733, Epsilon: 0.4351424010585501\n",
      "Episode 166, Reward: -190.057246755474, Epsilon: 0.43296668905325736\n",
      "Episode 167, Reward: 1.644447259228059, Epsilon: 0.43080185560799106\n",
      "Episode 168, Reward: 0.33116039711462975, Epsilon: 0.4286478463299511\n",
      "Episode 169, Reward: 52.48832094492994, Epsilon: 0.42650460709830135\n",
      "Episode 170, Reward: -19.438455958801754, Epsilon: 0.42437208406280985\n",
      "Episode 171, Reward: -33.36117182313747, Epsilon: 0.4222502236424958\n",
      "Episode 172, Reward: -20.50376468885753, Epsilon: 0.42013897252428334\n",
      "Episode 173, Reward: -53.983078740996106, Epsilon: 0.4180382776616619\n",
      "Episode 174, Reward: -409.33784486536297, Epsilon: 0.4159480862733536\n",
      "Episode 175, Reward: -35.885457541756494, Epsilon: 0.41386834584198684\n",
      "Episode 176, Reward: -96.88890143248572, Epsilon: 0.4117990041127769\n",
      "Episode 177, Reward: -19.715032714717154, Epsilon: 0.40974000909221303\n",
      "Episode 178, Reward: -32.040623069546484, Epsilon: 0.40769130904675194\n",
      "Episode 179, Reward: 10.858017089199535, Epsilon: 0.40565285250151817\n",
      "Episode 180, Reward: -245.9222041945025, Epsilon: 0.4036245882390106\n",
      "Episode 181, Reward: 2.500879833380992, Epsilon: 0.4016064652978155\n",
      "Episode 182, Reward: -25.50327174098402, Epsilon: 0.3995984329713264\n",
      "Episode 183, Reward: 20.96061781848981, Epsilon: 0.3976004408064698\n",
      "Episode 184, Reward: -38.7307914590299, Epsilon: 0.39561243860243744\n",
      "Episode 185, Reward: 21.772942768217376, Epsilon: 0.3936343764094253\n",
      "Episode 186, Reward: -8.173077227141931, Epsilon: 0.39166620452737816\n",
      "Episode 187, Reward: -30.880027754059185, Epsilon: 0.3897078735047413\n",
      "Episode 188, Reward: -5.652023607274742, Epsilon: 0.3877593341372176\n",
      "Episode 189, Reward: 15.677973440917455, Epsilon: 0.3858205374665315\n",
      "Episode 190, Reward: 11.915531182507593, Epsilon: 0.38389143477919885\n",
      "Episode 191, Reward: 0.23216208680082673, Epsilon: 0.3819719776053028\n",
      "Episode 192, Reward: -26.160792350102582, Epsilon: 0.3800621177172763\n",
      "Episode 193, Reward: -20.45062703116062, Epsilon: 0.37816180712868996\n",
      "Episode 194, Reward: -128.6233626717595, Epsilon: 0.37627099809304654\n",
      "Episode 195, Reward: -55.58048085769663, Epsilon: 0.3743896431025813\n",
      "Episode 196, Reward: -28.633250197196844, Epsilon: 0.37251769488706843\n",
      "Episode 197, Reward: -369.0887272298407, Epsilon: 0.3706551064126331\n",
      "Episode 198, Reward: 52.055547901072174, Epsilon: 0.36880183088056995\n",
      "Episode 199, Reward: -28.720346213203683, Epsilon: 0.3669578217261671\n",
      "Episode 200, Reward: -46.11017374916274, Epsilon: 0.36512303261753626\n",
      "Checkpoint saved at episode 200\n",
      "Episode 201, Reward: -53.198231649251646, Epsilon: 0.3632974174544486\n",
      "Episode 202, Reward: -100.16938932323595, Epsilon: 0.3614809303671764\n",
      "Episode 203, Reward: -6.093453509022606, Epsilon: 0.3596735257153405\n",
      "Episode 204, Reward: -49.87401484940864, Epsilon: 0.3578751580867638\n",
      "Episode 205, Reward: 12.354045101627765, Epsilon: 0.35608578229633\n",
      "Episode 206, Reward: -55.17666093901069, Epsilon: 0.3543053533848483\n",
      "Episode 207, Reward: -44.065449525518716, Epsilon: 0.35253382661792404\n",
      "Episode 208, Reward: -63.35192574535048, Epsilon: 0.3507711574848344\n",
      "Episode 209, Reward: 50.87398079214799, Epsilon: 0.34901730169741024\n",
      "Episode 210, Reward: 26.857124591715163, Epsilon: 0.3472722151889232\n",
      "Episode 211, Reward: -10.851511724456163, Epsilon: 0.3455358541129786\n",
      "Episode 212, Reward: -26.984053431805567, Epsilon: 0.3438081748424137\n",
      "Episode 213, Reward: -50.156324712735355, Epsilon: 0.3420891339682016\n",
      "Episode 214, Reward: -12.529801742309331, Epsilon: 0.3403786882983606\n",
      "Episode 215, Reward: 9.354245874423484, Epsilon: 0.3386767948568688\n",
      "Episode 216, Reward: -4.354351192086445, Epsilon: 0.33698341088258443\n",
      "Episode 217, Reward: 7.850480019851901, Epsilon: 0.3352984938281715\n",
      "Episode 218, Reward: -32.68294185390265, Epsilon: 0.33362200135903064\n",
      "Episode 219, Reward: -4.147756183272605, Epsilon: 0.33195389135223546\n",
      "Episode 220, Reward: -94.26374733545421, Epsilon: 0.3302941218954743\n",
      "Episode 221, Reward: -44.12526662706419, Epsilon: 0.32864265128599696\n",
      "Episode 222, Reward: 9.227180553739188, Epsilon: 0.326999438029567\n",
      "Episode 223, Reward: 27.139379309290945, Epsilon: 0.3253644408394192\n",
      "Episode 224, Reward: -17.873231056016962, Epsilon: 0.3237376186352221\n",
      "Episode 225, Reward: 52.89176009343956, Epsilon: 0.322118930542046\n",
      "Episode 226, Reward: 42.03293245272886, Epsilon: 0.32050833588933575\n",
      "Episode 227, Reward: -31.96194562293161, Epsilon: 0.31890579420988907\n",
      "Episode 228, Reward: -18.24437818569821, Epsilon: 0.3173112652388396\n",
      "Episode 229, Reward: 52.16818840997486, Epsilon: 0.3157247089126454\n",
      "Episode 230, Reward: -95.51643000347535, Epsilon: 0.3141460853680822\n",
      "Episode 231, Reward: 67.79430933229877, Epsilon: 0.3125753549412418\n",
      "Episode 232, Reward: 88.484599060282, Epsilon: 0.31101247816653554\n",
      "Episode 233, Reward: 30.69607519173249, Epsilon: 0.30945741577570285\n",
      "Episode 234, Reward: 58.755893990804076, Epsilon: 0.3079101286968243\n",
      "Episode 235, Reward: 34.62803254545627, Epsilon: 0.3063705780533402\n",
      "Episode 236, Reward: 73.19192070632724, Epsilon: 0.30483872516307353\n",
      "Episode 237, Reward: 51.41062178336562, Epsilon: 0.3033145315372582\n",
      "Episode 238, Reward: 62.60166869596193, Epsilon: 0.3017979588795719\n",
      "Episode 239, Reward: -22.921667453727206, Epsilon: 0.30028896908517405\n",
      "Episode 240, Reward: 8.32241421920199, Epsilon: 0.2987875242397482\n",
      "Episode 241, Reward: 100.77031654967172, Epsilon: 0.29729358661854943\n",
      "Episode 242, Reward: 145.01461640541896, Epsilon: 0.29580711868545667\n",
      "Episode 243, Reward: -210.45032480191134, Epsilon: 0.2943280830920294\n",
      "Episode 244, Reward: 89.5937581982032, Epsilon: 0.29285644267656924\n",
      "Episode 245, Reward: 157.65640983003544, Epsilon: 0.2913921604631864\n",
      "Episode 246, Reward: 113.93592064469172, Epsilon: 0.28993519966087045\n",
      "Episode 247, Reward: 74.94089778736901, Epsilon: 0.2884855236625661\n",
      "Episode 248, Reward: 101.53702478384304, Epsilon: 0.28704309604425327\n",
      "Episode 249, Reward: 73.14152581195451, Epsilon: 0.285607880564032\n",
      "Episode 250, Reward: -3.1971142846000333, Epsilon: 0.28417984116121187\n",
      "Checkpoint saved at episode 250\n",
      "Episode 251, Reward: 286.7737662691477, Epsilon: 0.2827589419554058\n",
      "Episode 252, Reward: 5.855855382659172, Epsilon: 0.28134514724562876\n",
      "Episode 253, Reward: 178.46795088316134, Epsilon: 0.2799384215094006\n",
      "Episode 254, Reward: 38.42281566299246, Epsilon: 0.27853872940185365\n",
      "Episode 255, Reward: -32.84114188721398, Epsilon: 0.27714603575484437\n",
      "Episode 256, Reward: -3.450897356154371, Epsilon: 0.2757603055760701\n",
      "Episode 257, Reward: -27.423449186068694, Epsilon: 0.2743815040481898\n",
      "Episode 258, Reward: 273.24196986462846, Epsilon: 0.2730095965279488\n",
      "Episode 259, Reward: 3.0541251828811795, Epsilon: 0.27164454854530906\n",
      "Episode 260, Reward: 105.21659778197402, Epsilon: 0.2702863258025825\n",
      "Episode 261, Reward: 224.0586353290244, Epsilon: 0.2689348941735696\n",
      "Episode 262, Reward: 224.23834833473137, Epsilon: 0.26759021970270175\n",
      "Episode 263, Reward: -19.23344675100263, Epsilon: 0.2662522686041882\n",
      "Episode 264, Reward: -134.84446867682294, Epsilon: 0.2649210072611673\n",
      "Episode 265, Reward: -0.1528388478622702, Epsilon: 0.26359640222486147\n",
      "Episode 266, Reward: -1.4800802270573143, Epsilon: 0.26227842021373715\n",
      "Episode 267, Reward: 58.23473044550377, Epsilon: 0.2609670281126685\n",
      "Episode 268, Reward: 43.60749028124343, Epsilon: 0.25966219297210513\n",
      "Episode 269, Reward: 58.190964871122446, Epsilon: 0.2583638820072446\n",
      "Episode 270, Reward: 70.05724642836006, Epsilon: 0.2570720625972084\n",
      "Episode 271, Reward: 94.66909205120334, Epsilon: 0.25578670228422234\n",
      "Episode 272, Reward: 26.825912416107087, Epsilon: 0.25450776877280124\n",
      "Episode 273, Reward: 251.01951789250768, Epsilon: 0.2532352299289372\n",
      "Episode 274, Reward: -22.192287359605842, Epsilon: 0.2519690537792925\n",
      "Episode 275, Reward: 143.75521568069553, Epsilon: 0.2507092085103961\n",
      "Episode 276, Reward: 81.1786105994302, Epsilon: 0.2494556624678441\n",
      "Episode 277, Reward: 163.8617026113961, Epsilon: 0.24820838415550486\n",
      "Episode 278, Reward: -148.60404573031, Epsilon: 0.24696734223472733\n",
      "Episode 279, Reward: -11.143658410687152, Epsilon: 0.2457325055235537\n",
      "Episode 280, Reward: 239.47191910558183, Epsilon: 0.24450384299593592\n",
      "Episode 281, Reward: 44.81001484985137, Epsilon: 0.24328132378095624\n",
      "Episode 282, Reward: 22.923050670070765, Epsilon: 0.24206491716205145\n",
      "Episode 283, Reward: 150.64366303255548, Epsilon: 0.2408545925762412\n",
      "Episode 284, Reward: 39.14831521977129, Epsilon: 0.23965031961336\n",
      "Episode 285, Reward: 122.20575987567477, Epsilon: 0.2384520680152932\n",
      "Episode 286, Reward: -43.648123087555064, Epsilon: 0.23725980767521673\n",
      "Episode 287, Reward: 200.3099921116019, Epsilon: 0.23607350863684065\n",
      "Episode 288, Reward: -23.20280305590437, Epsilon: 0.23489314109365644\n",
      "Episode 289, Reward: 122.8560734952153, Epsilon: 0.23371867538818816\n",
      "Episode 290, Reward: 132.04375715457104, Epsilon: 0.23255008201124722\n",
      "Episode 291, Reward: 176.7613065484733, Epsilon: 0.231387331601191\n",
      "Episode 292, Reward: 130.37389741733887, Epsilon: 0.23023039494318503\n",
      "Episode 293, Reward: 25.855244212855666, Epsilon: 0.2290792429684691\n",
      "Episode 294, Reward: 24.054785372456067, Epsilon: 0.22793384675362674\n",
      "Episode 295, Reward: 19.366677441631253, Epsilon: 0.22679417751985861\n",
      "Episode 296, Reward: 284.32289999952525, Epsilon: 0.22566020663225933\n",
      "Episode 297, Reward: -12.075834688150806, Epsilon: 0.22453190559909803\n",
      "Episode 298, Reward: 137.57991988574037, Epsilon: 0.22340924607110255\n",
      "Episode 299, Reward: 242.20318452664216, Epsilon: 0.22229219984074702\n",
      "Episode 300, Reward: 251.75436430541035, Epsilon: 0.2211807388415433\n",
      "Checkpoint saved at episode 300\n",
      "Episode 301, Reward: -33.345076010160085, Epsilon: 0.22007483514733558\n",
      "Episode 302, Reward: -157.53047714231013, Epsilon: 0.2189744609715989\n",
      "Episode 303, Reward: 11.931579442213973, Epsilon: 0.2178795886667409\n",
      "Episode 304, Reward: 221.3575706032264, Epsilon: 0.2167901907234072\n",
      "Episode 305, Reward: 145.80418736936923, Epsilon: 0.21570623976979014\n",
      "Episode 306, Reward: 257.5661535037594, Epsilon: 0.21462770857094118\n",
      "Episode 307, Reward: 241.88808473079752, Epsilon: 0.21355457002808648\n",
      "Episode 308, Reward: 175.7344279376917, Epsilon: 0.21248679717794605\n",
      "True\n",
      "Training done\n",
      "Checkpoint saved at episode 308\n"
     ]
    }
   ],
   "source": [
    "for episode in range(num_episodes):\n",
    "    # Update target network periodically\n",
    "    if episode % update_freq == 0:\n",
    "        target_net.load_state_dict(main_net.state_dict())\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() < epsilon:\n",
    "            action = env.action_space.sample()  # Random action\n",
    "        else:\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            taus = torch.rand((1, num_quantiles), dtype=torch.float32).to(device)  # Sample quantile fractions\n",
    "            with torch.no_grad():\n",
    "                q_quantiles = main_net(state_tensor, taus)\n",
    "            q_values = q_quantiles.mean(dim=1)\n",
    "            action = q_values.argmax().item()  # Best action\n",
    "\n",
    "        # Take action and observe next state, reward, and done flag\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Store experience in replay buffer\n",
    "        replay_buffer.add_experience(Experience(state, action, reward, next_state, done))\n",
    "\n",
    "        # Update state and episode reward\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Sample minibatch from replay buffer\n",
    "        if len(replay_buffer.buffer) >= batch_size:\n",
    "            experiences = replay_buffer.sample(batch_size)\n",
    "\n",
    "            # Prepare minibatch tensors\n",
    "            states = torch.tensor(experiences.state, dtype=torch.float32).to(device)\n",
    "            actions = torch.tensor(experiences.action).unsqueeze(1).to(device)\n",
    "            rewards = torch.tensor(experiences.reward, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "            next_states = torch.tensor(experiences.next_state, dtype=torch.float32).to(device)\n",
    "            dones = torch.tensor(experiences.done, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "            # Sample quantile fractions\n",
    "            taus = torch.rand((batch_size, num_quantiles), dtype=torch.float32).to(device)\n",
    "\n",
    "            # Compute Q-values and target Q-values\n",
    "            q_quantiles = main_net(states, taus).gather(2, actions.unsqueeze(1).expand(-1, num_quantiles, -1)).squeeze(-1)\n",
    "            with torch.no_grad():\n",
    "                next_q_quantiles = target_net(next_states, taus)\n",
    "                next_q_values = next_q_quantiles.mean(dim=1)\n",
    "                next_actions = next_q_values.argmax(dim=1, keepdim=True)\n",
    "                target_quantiles = rewards + gamma * next_q_quantiles.gather(2, next_actions.unsqueeze(1).expand(-1, num_quantiles, -1)).squeeze(-1) * (1 - dones)\n",
    "\n",
    "            # Compute loss and update main network\n",
    "            loss = quantile_huber_loss(q_quantiles, target_quantiles, taus)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Decay epsilon\n",
    "    epsilon = max(epsilon_end, epsilon_decay * epsilon)\n",
    "\n",
    "\n",
    "\n",
    "    # Logging and monitoring\n",
    "    episode_rewards.append(episode_reward)\n",
    "    print(f\"Episode {episode}, Reward: {episode_reward}, Epsilon: {epsilon}\")\n",
    "\n",
    "    # Save model periodically\n",
    "    if episode % 50 == 0:\n",
    "        save_checkpoint({\n",
    "            'episode': episode,\n",
    "            'main_net_state_dict': main_net.state_dict(),\n",
    "            'target_net_state_dict': target_net.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'epsilon': epsilon\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved at episode {episode}\")\n",
    "\n",
    "    if(sum(episode_rewards[-5:])>1000):\n",
    "      print(sum(episode_rewards[-5:])>1000)\n",
    "      print(\"Training done\")\n",
    "      save_checkpoint({\n",
    "            'episode': episode,\n",
    "            'main_net_state_dict': main_net.state_dict(),\n",
    "            'target_net_state_dict': target_net.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'epsilon': epsilon\n",
    "        }, checkpoint_path)\n",
    "      print(f\"Checkpoint saved at episode {episode}\")\n",
    "      break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
