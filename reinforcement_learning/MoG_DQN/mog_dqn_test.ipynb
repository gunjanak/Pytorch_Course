{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janak/Documents/Pytorch_CPU/venv/lib/python3.10/site-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/home/janak/Documents/Pytorch_CPU/venv/lib/python3.10/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Lunar Lander environment\n",
    "env = gym.make('LunarLander-v2',render_mode='human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianMixtureModel(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim,num_components,hidden_dim=256):\n",
    "        super(GaussianMixtureModel, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.num_components = num_components\n",
    "\n",
    "        self.fc1 = nn.Linear(state_dim,hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim,hidden_dim)\n",
    "\n",
    "        #output action_dim * num_components   (4*5)\n",
    "        self.mean = nn.Linear(hidden_dim, action_dim * num_components)\n",
    "        self.log_var = nn.Linear(hidden_dim, action_dim * num_components)\n",
    "        self.logits = nn.Linear(hidden_dim, action_dim * num_components)\n",
    "\n",
    "    def forward(self, state):\n",
    "\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        #mean is reshaped from (1,20) to (1,4,5)and so is log_var and logits\n",
    "        mean = self.mean(x).view(-1, self.action_dim, self.num_components)\n",
    "        log_var = self.log_var(x).view(-1, self.action_dim, self.num_components)\n",
    "        log_var = torch.clamp(log_var, -10, 10)  # Clipping log variance for stability\n",
    "        logits = self.logits(x).view(-1, self.action_dim, self.num_components)\n",
    "        return mean, log_var, logits\n",
    "\n",
    "    def get_distribution(self, state):\n",
    "        \"\"\"\n",
    "        mean represents the expected value of the distribution.\n",
    "        log_var represents the logarithm of the variance of the distribution.\n",
    "        Taking the exponential of log_var yields the variance.\n",
    "        logits represent the unnormalized log probabilities of each action.\n",
    "        Softmax is applied to logits to convert them into a probability distribution.\n",
    "        This function encapsulates the logic of computing the distribution parameters\n",
    "        and is useful for sampling actions from the distribution or\n",
    "        computing probabilities of actions given states during training or\n",
    "        inference in reinforcement learning algorithms.\n",
    "        \"\"\"\n",
    "        mean, log_var, logits = self.forward(state)\n",
    "        return mean, torch.exp(log_var), torch.softmax(logits, dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experience replay buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = []\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def push(self, experience):\n",
    "        if len(self.buffer) >= self.capacity:\n",
    "            self.buffer.pop(0)\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        return [self.buffer[idx] for idx in indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(mean, var, logits, target):\n",
    "    # a Normal distribution object m is created using the provided mean and var tensors.\n",
    "    #This distribution will be used to calculate the log probabilities of the target values.\n",
    "\n",
    "    m = torch.distributions.Normal(mean, var)\n",
    "\n",
    "\n",
    "    #target.unsqueeze(-1) adds an extra dimension to target to make it compatible for broadcasting with mean.\n",
    "    #expand_as(mean) expands the target tensor to the same shape as mean.\n",
    "    #m.log_prob(target) computes the log probability of target under the normal distribution m.\n",
    "    #The result is a tensor of log probabilities with the same shape as mean.\n",
    "    log_prob = m.log_prob(target.unsqueeze(-1).expand_as(mean))\n",
    "    # print(f\"log_prob: {log_prob.shape}\")\n",
    "\n",
    "    #torch.sum(log_prob, dim=-2) sums the log probabilities along the second last dimension,\n",
    "    #aggregating the contributions across different dimensions of mean.\n",
    "    #torch.log(logits + 1e-10) adds the log of logits to the summed log probabilities.\n",
    "    # The 1e-10 is a small epsilon value added to avoid taking the logarithm of zero,\n",
    "    # which would result in numerical instability.\n",
    "    #The resulting log_prob tensor now incorporates both the log probabilities\n",
    "    #of the target values under the normal distributions and the log of the class logits.\n",
    "    log_prob = torch.sum(log_prob, dim=-2) + torch.log(logits + 1e-10)  # Adding epsilon for numerical stability\n",
    "    # print(f\"log_prob: {log_prob.shape}\")\n",
    "\n",
    "    #torch.logsumexp(log_prob, dim=-1) computes the log-sum-exp across the last dimension of log_prob.\n",
    "    # This operation is numerically stable and combines the probabilities in a way that prevents underflow or overflow.\n",
    "    #The negative sign indicates that we are converting the log-sum-exp to a negative log likelihood.\n",
    "    #.mean() computes the average loss over the batch.\n",
    "    loss = -torch.logsumexp(log_prob, dim=-1).mean()\n",
    "    # print(f\"loss: {loss.shape}\")\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename='checkpoint.pth'):\n",
    "    torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(filename='checkpoint.pth', map_location=None):\n",
    "    if map_location:\n",
    "        return torch.load(filename, map_location=map_location)\n",
    "    return torch.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "hidden_dim = 128\n",
    "num_components = 5\n",
    "learning_rate = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and target model\n",
    "model = GaussianMixtureModel(state_dim, action_dim, num_components,hidden_dim)\n",
    "target_model = GaussianMixtureModel(state_dim, action_dim, num_components,hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianMixtureModel(\n",
       "  (fc1): Linear(in_features=8, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (mean): Linear(in_features=128, out_features=20, bias=True)\n",
       "  (log_var): Linear(in_features=128, out_features=20, bias=True)\n",
       "  (logits): Linear(in_features=128, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move the model to the chosen device\n",
    "model.to(device)\n",
    "target_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = 'mog_dqn.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from episode 603\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    map_location = torch.device('cpu') if not torch.cuda.is_available() else None\n",
    "    checkpoint = load_checkpoint(checkpoint_path, map_location=map_location)\n",
    "    model.load_state_dict(checkpoint['main_net_state_dict'])\n",
    "    target_model.load_state_dict(checkpoint['target_net_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epsilon = checkpoint['epsilon']\n",
    "    start_episode = checkpoint['episode'] + 1\n",
    "    print(f\"Loaded checkpoint from episode {start_episode}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No checkpoint found, starting from scratch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianMixtureModel(\n",
       "  (fc1): Linear(in_features=8, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (mean): Linear(in_features=128, out_features=20, bias=True)\n",
       "  (log_var): Linear(in_features=128, out_features=20, bias=True)\n",
       "  (logits): Linear(in_features=128, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_model.load_state_dict(model.state_dict())\n",
    "target_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janak/Documents/Pytorch_CPU/venv/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Reward: 212.0231553219341\n",
      "Episode: 1, Reward: 147.40671003059174\n",
      "Episode: 2, Reward: 232.90812552693615\n",
      "Episode: 3, Reward: 90.22635322517452\n",
      "Episode: 4, Reward: 197.58630293074646\n",
      "Episode: 5, Reward: 146.569679007311\n",
      "Episode: 6, Reward: 192.26214516184638\n",
      "Episode: 7, Reward: 210.95717712649326\n",
      "Episode: 8, Reward: 144.8541496096244\n",
      "Episode: 9, Reward: 139.7599173840508\n"
     ]
    }
   ],
   "source": [
    "for episode in range(10):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "\n",
    "    while True:\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        mean, var, logits = model.get_distribution(state_tensor)\n",
    "\n",
    "        action_probs = mean.mean(dim=-1).softmax(dim=-1).detach().numpy()\n",
    "\n",
    "        # Handle NaN values in action_probs\n",
    "        if np.isnan(action_probs).any():\n",
    "            action_probs = np.nan_to_num(action_probs, nan=1.0/action_dim)\n",
    "            action_probs /= action_probs.sum()  # Re-normalize to ensure it's a valid probability distribution\n",
    "\n",
    "        action = np.argmax(action_probs[0])\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        \n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print(f\"Episode: {episode}, Reward: {episode_reward}\")\n",
    "\n",
    "    \n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
