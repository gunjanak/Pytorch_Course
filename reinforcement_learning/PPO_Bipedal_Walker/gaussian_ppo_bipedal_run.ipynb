{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributions as dist \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janak/Documents/Pytorch_CPU/venv/lib/python3.10/site-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import resource_stream, resource_exists\n",
      "/home/janak/Documents/Pytorch_CPU/venv/lib/python3.10/site-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/home/janak/Documents/Pytorch_CPU/venv/lib/python3.10/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "s_dim = env.observation_space.shape[0]\n",
    "a_dim = env.action_space.shape[0]\n",
    "print(s_dim)\n",
    "print(a_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Network without DiagGaussian\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(s_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mean = nn.Linear(128, a_dim)\n",
    "        self.logstd = nn.Parameter(torch.zeros(a_dim))  # Make logstd a learnable parameter\n",
    "\n",
    "    def forward(self, state, deterministic=False):\n",
    "        # Process the state through the network\n",
    "        # print(\"\\nInside PolicyNet:\")\n",
    "        # print(f\"Input State: {state}\")\n",
    "        feature = self.main(state)\n",
    "\n",
    "        # Compute mean and log standard deviation\n",
    "        mean = self.fc_mean(feature)\n",
    "        std = self.logstd.exp()  # Standard deviation is exp(logstd)\n",
    "\n",
    "        # Create a Normal distribution using PyTorch's default implementation\n",
    "        distribution = dist.Normal(mean, std)  # Renaming the variable to avoid conflict\n",
    "\n",
    "        if deterministic:\n",
    "            action = mean  # In a normal distribution, mode = mean\n",
    "        else:\n",
    "            action = distribution.sample()  # Sample an action from the distribution\n",
    "\n",
    "        log_prob = distribution.log_prob(action).sum(-1)  # Sum log probabilities across dimensions\n",
    "        return action, log_prob\n",
    "\n",
    "    def choose_action(self, state, deterministic=False):\n",
    "        feature = self.main(state)\n",
    "        mean = self.fc_mean(feature)\n",
    "        std = self.logstd.exp()\n",
    "        distribution = dist.Normal(mean, std)  # Renaming the variable to avoid conflict\n",
    "\n",
    "        if deterministic:\n",
    "            return mean\n",
    "        return distribution.sample()\n",
    "\n",
    "    def evaluate(self, state, action):\n",
    "        feature = self.main(state)\n",
    "        mean = self.fc_mean(feature)\n",
    "        std = self.logstd.exp()\n",
    "        distribution = dist.Normal(mean, std)  # Renaming the variable to avoid conflict\n",
    "\n",
    "        log_prob = distribution.log_prob(action).sum(-1)  # Sum across dimensions\n",
    "        entropy = distribution.entropy().sum(-1)  # Sum the entropies across dimensions\n",
    "        return log_prob, entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Value Network\n",
    "class ValueNet(nn.Module):\n",
    "    #Constructor\n",
    "    def __init__(self, s_dim):\n",
    "        super(ValueNet, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(s_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    #Forward pass\n",
    "    def forward(self, state):\n",
    "        return self.main(state)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = PolicyNet(s_dim, a_dim)\n",
    "value_net = ValueNet(s_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, env, policy_net, value_net, s_dim, a_dim, gamma=0.99, lamb=0.95, max_step=2048, lr=1e-4, max_grad_norm=0.5, ent_weight=0.01, clip_val=0.2, sample_n_epoch=4, sample_mb_size=64, device='cpu'):\n",
    "        self.env = env\n",
    "        self.policy_net = policy_net\n",
    "        self.value_net = value_net\n",
    "        self.s_dim = s_dim\n",
    "        self.a_dim = a_dim\n",
    "        self.gamma = gamma\n",
    "        self.lamb = lamb\n",
    "        self.max_step = max_step\n",
    "        self.lr = lr\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.ent_weight = ent_weight\n",
    "        self.clip_val = clip_val\n",
    "        self.sample_n_epoch = sample_n_epoch\n",
    "        self.sample_mb_size = sample_mb_size\n",
    "        self.device = device\n",
    "\n",
    "        # Initialize optimizers for policy and value networks\n",
    "        self.opt_policy = torch.optim.Adam(self.policy_net.parameters(), lr)\n",
    "        self.opt_value = torch.optim.Adam(self.value_net.parameters(), lr)\n",
    "\n",
    "        # Storages for rollout data\n",
    "        self.mb_states = np.zeros((self.max_step, self.s_dim), dtype=np.float32)\n",
    "        self.mb_actions = np.zeros((self.max_step, self.a_dim), dtype=np.float32)\n",
    "        self.mb_values = np.zeros((self.max_step,), dtype=np.float32)\n",
    "        self.mb_rewards = np.zeros((self.max_step,), dtype=np.float32)\n",
    "        self.mb_a_logps = np.zeros((self.max_step,), dtype=np.float32)\n",
    "\n",
    "        # Lists to store metrics for tracking\n",
    "        self.episode_rewards = []\n",
    "        self.policy_losses = []\n",
    "        self.value_losses = []\n",
    "        self.entropies = []\n",
    "\n",
    "    # Compute discounted returns and GAE (same as original code)\n",
    "    def compute_discounted_return(self, rewards, last_value):\n",
    "        returns = np.zeros_like(rewards)\n",
    "        n_step = len(rewards)\n",
    "        for t in reversed(range(n_step)):\n",
    "            if t == n_step - 1:\n",
    "                returns[t] = rewards[t] + self.gamma * last_value\n",
    "            else:\n",
    "                returns[t] = rewards[t] + self.gamma * returns[t + 1]\n",
    "        return returns\n",
    "\n",
    "    def compute_gae(self, rewards, values, last_value):\n",
    "        advs = np.zeros_like(rewards)\n",
    "        n_step = len(rewards)\n",
    "        last_gae_lam = 0.0\n",
    "        for t in reversed(range(n_step)):\n",
    "            if t == n_step - 1:\n",
    "                next_value = last_value\n",
    "            else:\n",
    "                next_value = values[t + 1]\n",
    "            delta = rewards[t] + self.gamma * next_value - values[t]\n",
    "            advs[t] = last_gae_lam = delta + self.gamma * self.lamb * last_gae_lam\n",
    "        return advs + values\n",
    "\n",
    "    def run_episode(self):\n",
    "        state = self.env.reset()\n",
    "        episode_len = self.max_step\n",
    "\n",
    "        for step in range(self.max_step):\n",
    "            state_tensor = torch.tensor(np.expand_dims(state, axis=0), dtype=torch.float32, device=self.device)\n",
    "            action, a_logp = self.policy_net(state_tensor)\n",
    "            value = self.value_net(state_tensor)\n",
    "\n",
    "            action = action.cpu().numpy()[0]\n",
    "            a_logp = a_logp.cpu().numpy()\n",
    "            value = value.cpu().numpy()\n",
    "\n",
    "            self.mb_states[step] = state\n",
    "            self.mb_actions[step] = action\n",
    "            self.mb_a_logps[step] = a_logp\n",
    "            self.mb_values[step] = value\n",
    "\n",
    "            state, reward, done, _ = self.env.step(action)\n",
    "            self.mb_rewards[step] = reward\n",
    "\n",
    "            if done:\n",
    "                episode_len = step + 1\n",
    "                break\n",
    "\n",
    "        last_value = self.value_net(torch.tensor(np.expand_dims(state, axis=0), dtype=torch.float32, device=self.device)).cpu().numpy()\n",
    "        # mb_returns = self.compute_discounted_return(self.mb_rewards[:episode_len], last_value)\n",
    "        # mb_advs = mb_returns - self.mb_values[:episode_len]\n",
    "        # mb_advs = (mb_advs - mb_advs.mean()) / (mb_advs.std() + 1e-6)\n",
    "\n",
    "        mb_returns = self.compute_gae(self.mb_rewards[:episode_len], self.mb_values[:episode_len], last_value)\n",
    "        mb_advs = mb_returns - self.mb_values[:episode_len]\n",
    "        mb_advs = (mb_advs - mb_advs.mean()) / (mb_advs.std() + 1e-6)\n",
    "\n",
    "\n",
    "        return self.mb_states[:episode_len], self.mb_actions[:episode_len], self.mb_a_logps[:episode_len], self.mb_values[:episode_len], mb_returns, mb_advs, self.mb_rewards[:episode_len]\n",
    "\n",
    "    def ppo_update(self, mb_states, mb_actions, mb_values, mb_advs, mb_returns, mb_a_logps):\n",
    "        # Convert rollout data to tensors\n",
    "        mb_states = torch.from_numpy(mb_states).to(self.device)\n",
    "        mb_actions = torch.from_numpy(mb_actions).to(self.device)\n",
    "        mb_values = torch.from_numpy(mb_values).to(self.device)\n",
    "        mb_advs = torch.from_numpy(mb_advs).to(self.device)\n",
    "        mb_returns = torch.from_numpy(mb_returns).to(self.device)\n",
    "        mb_a_logps = torch.from_numpy(mb_a_logps).to(self.device)\n",
    "\n",
    "        episode_length = len(mb_states)\n",
    "        rand_idx = np.arange(episode_length)\n",
    "        sample_n_mb = episode_length // self.sample_mb_size\n",
    "\n",
    "        if sample_n_mb <= 0:\n",
    "            sample_mb_size = episode_length\n",
    "            sample_n_mb = 1\n",
    "        else:\n",
    "            sample_mb_size = self.sample_mb_size\n",
    "\n",
    "        for i in range(self.sample_n_epoch):\n",
    "            np.random.shuffle(rand_idx)\n",
    "            for j in range(sample_n_mb):\n",
    "                sample_idx = rand_idx[j * sample_mb_size: (j + 1) * sample_mb_size]\n",
    "                sample_states = mb_states[sample_idx]\n",
    "                sample_actions = mb_actions[sample_idx]\n",
    "                sample_old_values = mb_values[sample_idx]\n",
    "                sample_advs = mb_advs[sample_idx]\n",
    "                sample_returns = mb_returns[sample_idx]\n",
    "                sample_old_a_logps = mb_a_logps[sample_idx]\n",
    "\n",
    "                # Get policy log probabilities and entropy\n",
    "                sample_a_logps, sample_ents = self.policy_net.evaluate(sample_states, sample_actions)\n",
    "                sample_values = self.value_net(sample_states)\n",
    "                ent = sample_ents.mean()\n",
    "\n",
    "                # Value loss\n",
    "                v_pred_clip = sample_old_values + torch.clamp(sample_values - sample_old_values, -self.clip_val, self.clip_val)\n",
    "                v_loss1 = (sample_returns - sample_values).pow(2)\n",
    "                v_loss2 = (sample_returns - v_pred_clip).pow(2)\n",
    "                v_loss = torch.max(v_loss1, v_loss2).mean()\n",
    "\n",
    "                # Policy loss\n",
    "                ratio = (sample_a_logps - sample_old_a_logps).exp()\n",
    "                pg_loss1 = -sample_advs * ratio\n",
    "                pg_loss2 = -sample_advs * torch.clamp(ratio, 1.0 - self.clip_val, 1.0 + self.clip_val)\n",
    "                pg_loss = torch.max(pg_loss1, pg_loss2).mean() - self.ent_weight * ent\n",
    "\n",
    "                # Train actor (policy network)\n",
    "                self.opt_policy.zero_grad()\n",
    "                pg_loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.policy_net.parameters(), self.max_grad_norm)\n",
    "                self.opt_policy.step()\n",
    "\n",
    "                # Train critic (value network)\n",
    "                self.opt_value.zero_grad()\n",
    "                v_loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.value_net.parameters(), self.max_grad_norm)\n",
    "                self.opt_value.step()\n",
    "\n",
    "        return pg_loss.item(), v_loss.item(), ent.item()\n",
    "\n",
    "\n",
    "\n",
    "    def save_metrics(self):\n",
    "\n",
    "\n",
    "        save_path = 'gaussian_ppo_bipedal.pt'\n",
    "        torch.save({\n",
    "\n",
    "            'policy_net_state_dict': self.policy_net.state_dict(),\n",
    "            'value_net_state_dict': self.value_net.state_dict(),\n",
    "            'episode_rewards': self.episode_rewards,\n",
    "            'policy_losses': self.policy_losses,\n",
    "            'value_losses': self.value_losses,\n",
    "            'entropies': self.entropies\n",
    "        }, save_path)\n",
    "        print(f\"Metrics and model saved at {save_path}\")\n",
    "\n",
    "    def load_model(self, load_dir='gaussian_ppo_bipedal.pt', device=None):\n",
    "        if not os.path.exists(load_dir):\n",
    "            print(f\"No model found at {load_dir}\")\n",
    "            return\n",
    "\n",
    "        # Load checkpoint (device-aware loading)\n",
    "        checkpoint = torch.load(load_dir, map_location=device or self.device)\n",
    "\n",
    "        # Load state_dicts for networks\n",
    "        self.policy_net.load_state_dict(checkpoint['policy_net_state_dict'])\n",
    "        self.value_net.load_state_dict(checkpoint['value_net_state_dict'])\n",
    "\n",
    "        # Load training metrics\n",
    "        self.episode_rewards = checkpoint.get('episode_rewards', [])\n",
    "        self.policy_losses = checkpoint.get('policy_losses', [])\n",
    "        self.value_losses = checkpoint.get('value_losses', [])\n",
    "        self.entropies = checkpoint.get('entropies', [])\n",
    "\n",
    "        print(f\"Model and metrics loaded from {load_dir}\")\n",
    "\n",
    "    def moving_average(self, data, window_size=10):\n",
    "        \"\"\"Compute moving average for smooth plotting.\"\"\"\n",
    "        cumsum = np.cumsum(np.insert(data, 0, 0))\n",
    "        return (cumsum[window_size:] - cumsum[:-window_size]) / window_size\n",
    "\n",
    "    def plot_metrics(self, window_size=100):\n",
    "        episodes = range(len(self.episode_rewards))\n",
    "\n",
    "        # Calculate moving averages\n",
    "        ma_rewards = self.moving_average(self.episode_rewards, window_size)\n",
    "        ma_policy_losses = self.moving_average(self.policy_losses, window_size)\n",
    "        ma_value_losses = self.moving_average(self.value_losses, window_size)\n",
    "        ma_entropies = self.moving_average(self.entropies, window_size)\n",
    "\n",
    "        # Plotting\n",
    "        fig, axs = plt.subplots(4, 1, figsize=(10, 15))\n",
    "\n",
    "        # Total Rewards per Episode\n",
    "        axs[0].plot(episodes, self.episode_rewards, label=\"Total Reward\", color='blue', alpha=0.5)\n",
    "        axs[0].plot(episodes[window_size-1:], ma_rewards, label=f\"Moving Avg ({window_size})\", color='blue')\n",
    "        axs[0].set_title(\"Total Rewards per Episode\")\n",
    "        axs[0].set_xlabel(\"Episode\")\n",
    "        axs[0].set_ylabel(\"Reward\")\n",
    "        axs[0].legend()\n",
    "\n",
    "        # Policy Loss\n",
    "        axs[1].plot(episodes, self.policy_losses, label=\"Policy Loss\", color='red', alpha=0.5)\n",
    "        axs[1].plot(episodes[window_size-1:], ma_policy_losses, label=f\"Moving Avg ({window_size})\", color='red')\n",
    "        axs[1].set_title(\"Policy Loss per Episode\")\n",
    "        axs[1].set_xlabel(\"Episode\")\n",
    "        axs[1].set_ylabel(\"Policy Loss\")\n",
    "        axs[1].legend()\n",
    "\n",
    "        # Value Loss\n",
    "        axs[2].plot(episodes, self.value_losses, label=\"Value Loss\", color='green', alpha=0.5)\n",
    "        axs[2].plot(episodes[window_size-1:], ma_value_losses, label=f\"Moving Avg ({window_size})\", color='green')\n",
    "        axs[2].set_title(\"Value Loss per Episode\")\n",
    "        axs[2].set_xlabel(\"Episode\")\n",
    "        axs[2].set_ylabel(\"Value Loss\")\n",
    "        axs[2].legend()\n",
    "\n",
    "        # Entropy\n",
    "        axs[3].plot(episodes, self.entropies, label=\"Entropy\", color='purple', alpha=0.5)\n",
    "        axs[3].plot(episodes[window_size-1:], ma_entropies, label=f\"Moving Avg ({window_size})\", color='purple')\n",
    "        axs[3].set_title(\"Entropy per Episode\")\n",
    "        axs[3].set_xlabel(\"Episode\")\n",
    "        axs[3].set_ylabel(\"Entropy\")\n",
    "        axs[3].legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def train(self, max_episode=5000):\n",
    "        mean_total_reward = 0\n",
    "        mean_length = 0\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        for i in range(max_episode):\n",
    "            with torch.no_grad():\n",
    "                mb_states, mb_actions, mb_old_a_logps, mb_values, mb_returns, mb_advs, mb_rewards = self.run_episode()\n",
    "\n",
    "            # Train PPO agent\n",
    "            pg_loss, v_loss, ent = self.ppo_update(mb_states, mb_actions, mb_values, mb_advs, mb_returns, mb_old_a_logps)\n",
    "\n",
    "            # Track metrics\n",
    "            self.episode_rewards.append(mb_rewards.sum())\n",
    "            self.policy_losses.append(pg_loss)\n",
    "            self.value_losses.append(v_loss)\n",
    "            self.entropies.append(ent)\n",
    "\n",
    "            mean_total_reward += mb_rewards.sum()\n",
    "            mean_length += len(mb_states)\n",
    "\n",
    "            # Logging\n",
    "            print(\"[Episode {:4d}] total reward = {:.6f}, length = {:d}\".format(i, mb_rewards.sum(), len(mb_states)))\n",
    "\n",
    "\n",
    "            if i > 100 and sum(self.episode_rewards[-5:])>1200:\n",
    "                print(\"Training done\")\n",
    "                self.save_metrics()\n",
    "\n",
    "\n",
    "            if i % 200 == 0:\n",
    "                print(\"\\n[{:5d} / {:5d}]\".format(i, max_episode))\n",
    "                print(\"----------------------------------\")\n",
    "                print(\"actor loss = {:.6f}\".format(pg_loss))\n",
    "                print(\"critic loss = {:.6f}\".format(v_loss))\n",
    "                print(\"entropy = {:.6f}\".format(ent))\n",
    "                print(\"mean return = {:.6f}\".format(mean_total_reward / 200))\n",
    "                print(\"mean length = {:.2f}\".format(mean_length / 200))\n",
    "                print(\"\\nSaving the model ... \", end=\"\")\n",
    "                self.save_metrics()\n",
    "                mean_total_reward = 0\n",
    "                mean_length = 0\n",
    "\n",
    "    def test(self, test_episodes=10):\n",
    "        total_reward = 0\n",
    "\n",
    "        for episode in range(test_episodes):\n",
    "            state = self.env.reset()\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                state_tensor = torch.tensor(np.expand_dims(state, axis=0), dtype=torch.float32, device=self.device)\n",
    "                action, _ = self.policy_net(state_tensor)\n",
    "                action = action.cpu().numpy()[0]\n",
    "                state, reward, done, _ = self.env.step(action)\n",
    "                episode_reward += reward\n",
    "\n",
    "            total_reward += episode_reward\n",
    "            print(f\"Episode {episode + 1} Reward: {episode_reward:.6f}\")\n",
    "\n",
    "        avg_reward = total_reward / test_episodes\n",
    "        print(f\"Average Reward over {test_episodes} episodes: {avg_reward:.6f}\")\n",
    "        return avg_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_agent = PPO(env, policy_net, value_net, s_dim=s_dim, a_dim=a_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_112521/405693779.py:81: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  self.mb_a_logps[step] = a_logp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode    0] total reward = 272.570007, length = 890\n",
      "\n",
      "[    0 /   201]\n",
      "----------------------------------\n",
      "actor loss = -0.204788\n",
      "critic loss = 1.915187\n",
      "entropy = 5.247126\n",
      "mean return = 1.362850\n",
      "mean length = 4.45\n",
      "\n",
      "Saving the model ... Metrics and model saved at gaussian_ppo_bipedal.pt\n",
      "[Episode    1] total reward = 260.005493, length = 1025\n",
      "[Episode    2] total reward = 261.407440, length = 1012\n",
      "[Episode    3] total reward = 272.235962, length = 906\n",
      "[Episode    4] total reward = 273.623444, length = 898\n",
      "[Episode    5] total reward = 270.639496, length = 910\n",
      "[Episode    6] total reward = 273.568176, length = 883\n",
      "[Episode    7] total reward = 57.501686, length = 583\n",
      "[Episode    8] total reward = 273.025085, length = 893\n",
      "[Episode    9] total reward = 265.354858, length = 978\n",
      "[Episode   10] total reward = 17.867062, length = 465\n",
      "[Episode   11] total reward = 115.281479, length = 761\n",
      "[Episode   12] total reward = -108.542053, length = 83\n",
      "[Episode   13] total reward = -93.486359, length = 124\n",
      "[Episode   14] total reward = 270.777344, length = 919\n",
      "[Episode   15] total reward = 268.790344, length = 933\n",
      "[Episode   16] total reward = -108.932289, length = 45\n",
      "[Episode   17] total reward = 275.948608, length = 862\n",
      "[Episode   18] total reward = -67.574860, length = 289\n",
      "[Episode   19] total reward = -96.860054, length = 106\n",
      "[Episode   20] total reward = 267.595337, length = 940\n",
      "[Episode   21] total reward = 269.130798, length = 929\n",
      "[Episode   22] total reward = 161.597809, length = 969\n",
      "[Episode   23] total reward = 270.472656, length = 911\n",
      "[Episode   24] total reward = 270.000854, length = 915\n",
      "[Episode   25] total reward = 273.283600, length = 895\n",
      "[Episode   26] total reward = 274.578278, length = 871\n",
      "[Episode   27] total reward = 274.758881, length = 882\n",
      "[Episode   28] total reward = 271.149048, length = 920\n",
      "[Episode   29] total reward = 271.814392, length = 908\n",
      "[Episode   30] total reward = -114.627899, length = 92\n",
      "[Episode   31] total reward = -105.290291, length = 139\n",
      "[Episode   32] total reward = 12.333916, length = 412\n",
      "[Episode   33] total reward = 273.931946, length = 884\n",
      "[Episode   34] total reward = -112.705437, length = 43\n",
      "[Episode   35] total reward = -113.488930, length = 66\n",
      "[Episode   36] total reward = 268.803131, length = 930\n",
      "[Episode   37] total reward = 270.155884, length = 913\n",
      "[Episode   38] total reward = 69.450256, length = 587\n",
      "[Episode   39] total reward = 275.831451, length = 877\n",
      "[Episode   40] total reward = 74.231194, length = 626\n",
      "[Episode   41] total reward = 273.192688, length = 895\n",
      "[Episode   42] total reward = -113.211533, length = 90\n",
      "[Episode   43] total reward = -107.876160, length = 41\n",
      "[Episode   44] total reward = -117.299004, length = 58\n",
      "[Episode   45] total reward = -120.944672, length = 108\n",
      "[Episode   46] total reward = -109.470596, length = 45\n",
      "[Episode   47] total reward = -108.234695, length = 69\n",
      "[Episode   48] total reward = 271.275543, length = 893\n",
      "[Episode   49] total reward = 269.920380, length = 909\n",
      "[Episode   50] total reward = -107.683914, length = 101\n",
      "[Episode   51] total reward = 273.012115, length = 884\n",
      "[Episode   52] total reward = 271.741394, length = 910\n",
      "[Episode   53] total reward = -106.207596, length = 91\n",
      "[Episode   54] total reward = 275.168549, length = 871\n",
      "[Episode   55] total reward = 270.395935, length = 911\n",
      "[Episode   56] total reward = 273.426147, length = 884\n",
      "[Episode   57] total reward = 274.895264, length = 854\n",
      "[Episode   58] total reward = -114.555229, length = 97\n",
      "[Episode   59] total reward = 134.403656, length = 802\n",
      "[Episode   60] total reward = -112.255058, length = 52\n",
      "[Episode   61] total reward = 271.964111, length = 898\n",
      "[Episode   62] total reward = -107.411842, length = 43\n",
      "[Episode   63] total reward = -114.912910, length = 75\n",
      "[Episode   64] total reward = -110.816467, length = 61\n",
      "[Episode   65] total reward = -113.951645, length = 75\n",
      "[Episode   66] total reward = -112.258881, length = 68\n",
      "[Episode   67] total reward = -112.420540, length = 53\n",
      "[Episode   68] total reward = -42.665085, length = 266\n",
      "[Episode   69] total reward = -107.740921, length = 46\n",
      "[Episode   70] total reward = 77.874428, length = 649\n",
      "[Episode   71] total reward = -117.973839, length = 120\n",
      "[Episode   72] total reward = -6.999283, length = 345\n",
      "[Episode   73] total reward = 128.073456, length = 750\n",
      "[Episode   74] total reward = -86.818634, length = 148\n",
      "[Episode   75] total reward = -112.977242, length = 53\n",
      "[Episode   76] total reward = -115.185593, length = 85\n",
      "[Episode   77] total reward = -108.982857, length = 47\n",
      "[Episode   78] total reward = 270.490845, length = 903\n",
      "[Episode   79] total reward = 52.892208, length = 579\n",
      "[Episode   80] total reward = -110.164696, length = 44\n",
      "[Episode   81] total reward = 266.435059, length = 958\n",
      "[Episode   82] total reward = 269.666138, length = 923\n",
      "[Episode   83] total reward = -111.777534, length = 78\n",
      "[Episode   84] total reward = 274.351807, length = 881\n",
      "[Episode   85] total reward = -117.261360, length = 83\n",
      "[Episode   86] total reward = -108.900742, length = 43\n",
      "[Episode   87] total reward = 270.829102, length = 913\n",
      "[Episode   88] total reward = -100.689789, length = 173\n",
      "[Episode   89] total reward = -110.176781, length = 72\n",
      "[Episode   90] total reward = 13.553574, length = 396\n",
      "[Episode   91] total reward = 272.805481, length = 907\n",
      "[Episode   92] total reward = 273.245605, length = 899\n",
      "[Episode   93] total reward = -120.666763, length = 89\n",
      "[Episode   94] total reward = -119.282021, length = 94\n",
      "[Episode   95] total reward = 14.559616, length = 425\n",
      "[Episode   96] total reward = -7.234230, length = 378\n",
      "[Episode   97] total reward = 273.347351, length = 893\n",
      "[Episode   98] total reward = -106.798637, length = 70\n",
      "[Episode   99] total reward = 97.702904, length = 644\n",
      "[Episode  100] total reward = 276.550842, length = 868\n",
      "[Episode  101] total reward = -66.714317, length = 318\n",
      "[Episode  102] total reward = -92.272591, length = 176\n",
      "[Episode  103] total reward = -111.750610, length = 67\n",
      "[Episode  104] total reward = -100.702995, length = 117\n",
      "[Episode  105] total reward = 273.691895, length = 907\n",
      "[Episode  106] total reward = -40.568832, length = 262\n",
      "[Episode  107] total reward = -101.369621, length = 109\n",
      "[Episode  108] total reward = 84.438179, length = 643\n",
      "[Episode  109] total reward = -101.780907, length = 196\n",
      "[Episode  110] total reward = 41.621620, length = 508\n",
      "[Episode  111] total reward = 48.507179, length = 562\n",
      "[Episode  112] total reward = -115.502174, length = 76\n",
      "[Episode  113] total reward = 276.360321, length = 864\n",
      "[Episode  114] total reward = -105.643188, length = 83\n",
      "[Episode  115] total reward = 275.019623, length = 883\n",
      "[Episode  116] total reward = -111.806190, length = 80\n",
      "[Episode  117] total reward = -105.615860, length = 169\n",
      "[Episode  118] total reward = 274.604431, length = 891\n",
      "[Episode  119] total reward = -113.036278, length = 93\n",
      "[Episode  120] total reward = -107.959877, length = 64\n",
      "[Episode  121] total reward = 278.571167, length = 849\n",
      "[Episode  122] total reward = 277.900085, length = 846\n",
      "[Episode  123] total reward = -106.440140, length = 75\n",
      "[Episode  124] total reward = 26.366333, length = 495\n",
      "[Episode  125] total reward = -86.100395, length = 212\n",
      "[Episode  126] total reward = -111.028259, length = 77\n",
      "[Episode  127] total reward = -114.754623, length = 107\n",
      "[Episode  128] total reward = -110.356941, length = 45\n",
      "[Episode  129] total reward = 95.668808, length = 749\n",
      "[Episode  130] total reward = -109.416237, length = 43\n",
      "[Episode  131] total reward = -109.510925, length = 88\n",
      "[Episode  132] total reward = -108.143044, length = 72\n",
      "[Episode  133] total reward = 275.652344, length = 877\n",
      "[Episode  134] total reward = -105.040428, length = 133\n",
      "[Episode  135] total reward = 280.687866, length = 835\n",
      "[Episode  136] total reward = 274.860413, length = 883\n",
      "[Episode  137] total reward = -110.023132, length = 65\n",
      "[Episode  138] total reward = -120.635956, length = 118\n",
      "[Episode  139] total reward = -47.555218, length = 262\n",
      "[Episode  140] total reward = -108.258774, length = 42\n",
      "[Episode  141] total reward = -81.909683, length = 190\n",
      "[Episode  142] total reward = -109.291893, length = 71\n",
      "[Episode  143] total reward = 274.943665, length = 877\n",
      "[Episode  144] total reward = 271.830139, length = 904\n",
      "[Episode  145] total reward = 272.332153, length = 925\n",
      "[Episode  146] total reward = -103.930901, length = 89\n",
      "[Episode  147] total reward = -105.611877, length = 129\n",
      "[Episode  148] total reward = -111.648094, length = 69\n",
      "[Episode  149] total reward = 277.141602, length = 851\n",
      "[Episode  150] total reward = -113.819542, length = 130\n",
      "[Episode  151] total reward = 275.646271, length = 853\n",
      "[Episode  152] total reward = 275.969299, length = 855\n",
      "[Episode  153] total reward = 276.273071, length = 853\n",
      "[Episode  154] total reward = -114.581383, length = 105\n",
      "[Episode  155] total reward = 276.414673, length = 859\n",
      "[Episode  156] total reward = 272.240784, length = 847\n",
      "[Episode  157] total reward = 88.364273, length = 615\n",
      "[Episode  158] total reward = -119.905106, length = 131\n",
      "[Episode  159] total reward = 270.902710, length = 910\n",
      "[Episode  160] total reward = 57.664242, length = 552\n",
      "[Episode  161] total reward = 275.910553, length = 854\n",
      "[Episode  162] total reward = -10.448097, length = 337\n",
      "[Episode  163] total reward = -114.678841, length = 58\n",
      "[Episode  164] total reward = -108.314957, length = 115\n",
      "[Episode  165] total reward = -108.510292, length = 73\n",
      "[Episode  166] total reward = -15.854555, length = 327\n",
      "[Episode  167] total reward = 77.048645, length = 578\n",
      "[Episode  168] total reward = 15.195850, length = 450\n",
      "[Episode  169] total reward = 151.213211, length = 773\n",
      "[Episode  170] total reward = 124.572067, length = 760\n",
      "[Episode  171] total reward = 275.948059, length = 860\n",
      "[Episode  172] total reward = 16.013390, length = 407\n",
      "[Episode  173] total reward = 273.950409, length = 855\n",
      "[Episode  174] total reward = -111.829514, length = 63\n",
      "[Episode  175] total reward = -115.357750, length = 59\n",
      "[Episode  176] total reward = -107.241722, length = 139\n",
      "[Episode  177] total reward = -115.268471, length = 86\n",
      "[Episode  178] total reward = 277.759521, length = 835\n",
      "[Episode  179] total reward = -89.083031, length = 178\n",
      "[Episode  180] total reward = -116.758698, length = 118\n",
      "[Episode  181] total reward = -34.098610, length = 341\n",
      "[Episode  182] total reward = -109.461143, length = 39\n",
      "[Episode  183] total reward = 13.223259, length = 476\n",
      "[Episode  184] total reward = 275.760925, length = 862\n",
      "[Episode  185] total reward = -100.632607, length = 134\n",
      "[Episode  186] total reward = -108.882057, length = 43\n",
      "[Episode  187] total reward = -95.515205, length = 117\n",
      "[Episode  188] total reward = -107.769661, length = 38\n",
      "[Episode  189] total reward = -109.415077, length = 48\n",
      "[Episode  190] total reward = 273.700348, length = 870\n",
      "[Episode  191] total reward = 277.609619, length = 849\n",
      "[Episode  192] total reward = -111.983360, length = 68\n",
      "[Episode  193] total reward = -116.166275, length = 86\n",
      "[Episode  194] total reward = -109.731102, length = 76\n",
      "[Episode  195] total reward = -116.331848, length = 102\n",
      "[Episode  196] total reward = -107.014130, length = 50\n",
      "[Episode  197] total reward = -109.431442, length = 43\n",
      "[Episode  198] total reward = -111.002251, length = 82\n",
      "[Episode  199] total reward = -109.791283, length = 45\n",
      "[Episode  200] total reward = -109.383842, length = 47\n",
      "\n",
      "[  200 /   201]\n",
      "----------------------------------\n",
      "actor loss = -0.085687\n",
      "critic loss = 6890.919434\n",
      "entropy = 5.343359\n",
      "mean return = 50.562712\n",
      "mean length = 445.70\n",
      "\n",
      "Saving the model ... Metrics and model saved at gaussian_ppo_bipedal.pt\n"
     ]
    }
   ],
   "source": [
    "ppo_agent.train(max_episode=201)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Redefine and reload the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and metrics loaded from gaussian_ppo_bipedal.pt\n",
      "Episode 1 Reward: -34.316991\n",
      "Episode 2 Reward: 58.945596\n",
      "Episode 3 Reward: -110.614780\n",
      "Episode 4 Reward: -98.694248\n",
      "Episode 5 Reward: 268.900564\n",
      "Average Reward over 5 episodes: 16.844028\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\",render_mode='human')\n",
    "s_dim = env.observation_space.shape[0]\n",
    "a_dim = env.action_space.shape[0]\n",
    "policy_net = PolicyNet(s_dim, a_dim)\n",
    "value_net = ValueNet(s_dim)\n",
    "\n",
    "ppo_agent = PPO(env, policy_net, value_net, s_dim=s_dim, a_dim=a_dim)\n",
    "ppo_agent.load_model()\n",
    "ppo_agent.test(test_episodes=5)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
