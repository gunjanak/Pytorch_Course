{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributions as dist \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janak/Documents/Pytorch_CPU/venv/lib/python3.10/site-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import resource_stream, resource_exists\n",
      "/home/janak/Documents/Pytorch_CPU/venv/lib/python3.10/site-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/home/janak/Documents/Pytorch_CPU/venv/lib/python3.10/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "s_dim = env.observation_space.shape[0]\n",
    "a_dim = env.action_space.shape[0]\n",
    "print(s_dim)\n",
    "print(a_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Network without DiagGaussian\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(s_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mean = nn.Linear(128, a_dim)\n",
    "        self.logstd = nn.Parameter(torch.zeros(a_dim))  # Make logstd a learnable parameter\n",
    "\n",
    "    def forward(self, state, deterministic=False):\n",
    "        # Process the state through the network\n",
    "        # print(\"\\nInside PolicyNet:\")\n",
    "        # print(f\"Input State: {state}\")\n",
    "        feature = self.main(state)\n",
    "\n",
    "        # Compute mean and log standard deviation\n",
    "        mean = self.fc_mean(feature)\n",
    "        std = self.logstd.exp()  # Standard deviation is exp(logstd)\n",
    "\n",
    "        # Create a Normal distribution using PyTorch's default implementation\n",
    "        distribution = dist.Normal(mean, std)  # Renaming the variable to avoid conflict\n",
    "\n",
    "        if deterministic:\n",
    "            action = mean  # In a normal distribution, mode = mean\n",
    "        else:\n",
    "            action = distribution.sample()  # Sample an action from the distribution\n",
    "\n",
    "        log_prob = distribution.log_prob(action).sum(-1)  # Sum log probabilities across dimensions\n",
    "        return action, log_prob\n",
    "\n",
    "    def choose_action(self, state, deterministic=False):\n",
    "        feature = self.main(state)\n",
    "        mean = self.fc_mean(feature)\n",
    "        std = self.logstd.exp()\n",
    "        distribution = dist.Normal(mean, std)  # Renaming the variable to avoid conflict\n",
    "\n",
    "        if deterministic:\n",
    "            return mean\n",
    "        return distribution.sample()\n",
    "\n",
    "    def evaluate(self, state, action):\n",
    "        feature = self.main(state)\n",
    "        mean = self.fc_mean(feature)\n",
    "        std = self.logstd.exp()\n",
    "        distribution = dist.Normal(mean, std)  # Renaming the variable to avoid conflict\n",
    "\n",
    "        log_prob = distribution.log_prob(action).sum(-1)  # Sum across dimensions\n",
    "        entropy = distribution.entropy().sum(-1)  # Sum the entropies across dimensions\n",
    "        return log_prob, entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Value Network\n",
    "class ValueNet(nn.Module):\n",
    "    #Constructor\n",
    "    def __init__(self, s_dim):\n",
    "        super(ValueNet, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(s_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    #Forward pass\n",
    "    def forward(self, state):\n",
    "        return self.main(state)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = PolicyNet(s_dim, a_dim)\n",
    "value_net = ValueNet(s_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, env, policy_net, value_net, s_dim, a_dim, gamma=0.99, lamb=0.95, max_step=2048, lr=1e-4, max_grad_norm=0.5, ent_weight=0.01, clip_val=0.2, sample_n_epoch=4, sample_mb_size=64, device='cpu'):\n",
    "        self.env = env\n",
    "        self.policy_net = policy_net\n",
    "        self.value_net = value_net\n",
    "        self.s_dim = s_dim\n",
    "        self.a_dim = a_dim\n",
    "        self.gamma = gamma\n",
    "        self.lamb = lamb\n",
    "        self.max_step = max_step\n",
    "        self.lr = lr\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.ent_weight = ent_weight\n",
    "        self.clip_val = clip_val\n",
    "        self.sample_n_epoch = sample_n_epoch\n",
    "        self.sample_mb_size = sample_mb_size\n",
    "        self.device = device\n",
    "\n",
    "        # Initialize optimizers for policy and value networks\n",
    "        self.opt_policy = torch.optim.Adam(self.policy_net.parameters(), lr)\n",
    "        self.opt_value = torch.optim.Adam(self.value_net.parameters(), lr)\n",
    "\n",
    "        # Storages for rollout data\n",
    "        self.mb_states = np.zeros((self.max_step, self.s_dim), dtype=np.float32)\n",
    "        self.mb_actions = np.zeros((self.max_step, self.a_dim), dtype=np.float32)\n",
    "        self.mb_values = np.zeros((self.max_step,), dtype=np.float32)\n",
    "        self.mb_rewards = np.zeros((self.max_step,), dtype=np.float32)\n",
    "        self.mb_a_logps = np.zeros((self.max_step,), dtype=np.float32)\n",
    "\n",
    "        # Lists to store metrics for tracking\n",
    "        self.episode_rewards = []\n",
    "        self.policy_losses = []\n",
    "        self.value_losses = []\n",
    "        self.entropies = []\n",
    "\n",
    "    # Compute discounted returns and GAE (same as original code)\n",
    "    def compute_discounted_return(self, rewards, last_value):\n",
    "        returns = np.zeros_like(rewards)\n",
    "        n_step = len(rewards)\n",
    "        for t in reversed(range(n_step)):\n",
    "            if t == n_step - 1:\n",
    "                returns[t] = rewards[t] + self.gamma * last_value\n",
    "            else:\n",
    "                returns[t] = rewards[t] + self.gamma * returns[t + 1]\n",
    "        return returns\n",
    "\n",
    "    def compute_gae(self, rewards, values, last_value):\n",
    "        advs = np.zeros_like(rewards)\n",
    "        n_step = len(rewards)\n",
    "        last_gae_lam = 0.0\n",
    "        for t in reversed(range(n_step)):\n",
    "            if t == n_step - 1:\n",
    "                next_value = last_value\n",
    "            else:\n",
    "                next_value = values[t + 1]\n",
    "            delta = rewards[t] + self.gamma * next_value - values[t]\n",
    "            advs[t] = last_gae_lam = delta + self.gamma * self.lamb * last_gae_lam\n",
    "        return advs + values\n",
    "\n",
    "    def run_episode(self):\n",
    "        state = self.env.reset()\n",
    "        episode_len = self.max_step\n",
    "\n",
    "        for step in range(self.max_step):\n",
    "            state_tensor = torch.tensor(np.expand_dims(state, axis=0), dtype=torch.float32, device=self.device)\n",
    "            action, a_logp = self.policy_net(state_tensor)\n",
    "            value = self.value_net(state_tensor)\n",
    "\n",
    "            action = action.cpu().numpy()[0]\n",
    "            a_logp = a_logp.cpu().numpy()\n",
    "            value = value.cpu().numpy()\n",
    "\n",
    "            self.mb_states[step] = state\n",
    "            self.mb_actions[step] = action\n",
    "            self.mb_a_logps[step] = a_logp\n",
    "            self.mb_values[step] = value\n",
    "\n",
    "            state, reward, done, _ = self.env.step(action)\n",
    "            self.mb_rewards[step] = reward\n",
    "\n",
    "            if done:\n",
    "                episode_len = step + 1\n",
    "                break\n",
    "\n",
    "        last_value = self.value_net(torch.tensor(np.expand_dims(state, axis=0), dtype=torch.float32, device=self.device)).cpu().numpy()\n",
    "        # mb_returns = self.compute_discounted_return(self.mb_rewards[:episode_len], last_value)\n",
    "        # mb_advs = mb_returns - self.mb_values[:episode_len]\n",
    "        # mb_advs = (mb_advs - mb_advs.mean()) / (mb_advs.std() + 1e-6)\n",
    "\n",
    "        mb_returns = self.compute_gae(self.mb_rewards[:episode_len], self.mb_values[:episode_len], last_value)\n",
    "        mb_advs = mb_returns - self.mb_values[:episode_len]\n",
    "        mb_advs = (mb_advs - mb_advs.mean()) / (mb_advs.std() + 1e-6)\n",
    "\n",
    "\n",
    "        return self.mb_states[:episode_len], self.mb_actions[:episode_len], self.mb_a_logps[:episode_len], self.mb_values[:episode_len], mb_returns, mb_advs, self.mb_rewards[:episode_len]\n",
    "\n",
    "    def ppo_update(self, mb_states, mb_actions, mb_values, mb_advs, mb_returns, mb_a_logps):\n",
    "        # Convert rollout data to tensors\n",
    "        mb_states = torch.from_numpy(mb_states).to(self.device)\n",
    "        mb_actions = torch.from_numpy(mb_actions).to(self.device)\n",
    "        mb_values = torch.from_numpy(mb_values).to(self.device)\n",
    "        mb_advs = torch.from_numpy(mb_advs).to(self.device)\n",
    "        mb_returns = torch.from_numpy(mb_returns).to(self.device)\n",
    "        mb_a_logps = torch.from_numpy(mb_a_logps).to(self.device)\n",
    "\n",
    "        episode_length = len(mb_states)\n",
    "        rand_idx = np.arange(episode_length)\n",
    "        sample_n_mb = episode_length // self.sample_mb_size\n",
    "\n",
    "        if sample_n_mb <= 0:\n",
    "            sample_mb_size = episode_length\n",
    "            sample_n_mb = 1\n",
    "        else:\n",
    "            sample_mb_size = self.sample_mb_size\n",
    "\n",
    "        for i in range(self.sample_n_epoch):\n",
    "            np.random.shuffle(rand_idx)\n",
    "            for j in range(sample_n_mb):\n",
    "                sample_idx = rand_idx[j * sample_mb_size: (j + 1) * sample_mb_size]\n",
    "                sample_states = mb_states[sample_idx]\n",
    "                sample_actions = mb_actions[sample_idx]\n",
    "                sample_old_values = mb_values[sample_idx]\n",
    "                sample_advs = mb_advs[sample_idx]\n",
    "                sample_returns = mb_returns[sample_idx]\n",
    "                sample_old_a_logps = mb_a_logps[sample_idx]\n",
    "\n",
    "                # Get policy log probabilities and entropy\n",
    "                sample_a_logps, sample_ents = self.policy_net.evaluate(sample_states, sample_actions)\n",
    "                sample_values = self.value_net(sample_states)\n",
    "                ent = sample_ents.mean()\n",
    "\n",
    "                # Value loss\n",
    "                v_pred_clip = sample_old_values + torch.clamp(sample_values - sample_old_values, -self.clip_val, self.clip_val)\n",
    "                v_loss1 = (sample_returns - sample_values).pow(2)\n",
    "                v_loss2 = (sample_returns - v_pred_clip).pow(2)\n",
    "                v_loss = torch.max(v_loss1, v_loss2).mean()\n",
    "\n",
    "                # Policy loss\n",
    "                ratio = (sample_a_logps - sample_old_a_logps).exp()\n",
    "                pg_loss1 = -sample_advs * ratio\n",
    "                pg_loss2 = -sample_advs * torch.clamp(ratio, 1.0 - self.clip_val, 1.0 + self.clip_val)\n",
    "                pg_loss = torch.max(pg_loss1, pg_loss2).mean() - self.ent_weight * ent\n",
    "\n",
    "                # Train actor (policy network)\n",
    "                self.opt_policy.zero_grad()\n",
    "                pg_loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.policy_net.parameters(), self.max_grad_norm)\n",
    "                self.opt_policy.step()\n",
    "\n",
    "                # Train critic (value network)\n",
    "                self.opt_value.zero_grad()\n",
    "                v_loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.value_net.parameters(), self.max_grad_norm)\n",
    "                self.opt_value.step()\n",
    "\n",
    "        return pg_loss.item(), v_loss.item(), ent.item()\n",
    "\n",
    "\n",
    "\n",
    "    def save_metrics(self):\n",
    "\n",
    "\n",
    "        save_path = 'gaussian_ppo_bipedal.pt'\n",
    "        torch.save({\n",
    "\n",
    "            'policy_net_state_dict': self.policy_net.state_dict(),\n",
    "            'value_net_state_dict': self.value_net.state_dict(),\n",
    "            'episode_rewards': self.episode_rewards,\n",
    "            'policy_losses': self.policy_losses,\n",
    "            'value_losses': self.value_losses,\n",
    "            'entropies': self.entropies\n",
    "        }, save_path)\n",
    "        print(f\"Metrics and model saved at {save_path}\")\n",
    "\n",
    "    def load_model(self, load_dir='gaussian_ppo_bipedal.pt', device=None):\n",
    "        if not os.path.exists(load_dir):\n",
    "            print(f\"No model found at {load_dir}\")\n",
    "            return\n",
    "\n",
    "        # Load checkpoint (device-aware loading)\n",
    "        checkpoint = torch.load(load_dir, map_location=device or self.device)\n",
    "\n",
    "        # Load state_dicts for networks\n",
    "        self.policy_net.load_state_dict(checkpoint['policy_net_state_dict'])\n",
    "        self.value_net.load_state_dict(checkpoint['value_net_state_dict'])\n",
    "\n",
    "        # Load training metrics\n",
    "        self.episode_rewards = checkpoint.get('episode_rewards', [])\n",
    "        self.policy_losses = checkpoint.get('policy_losses', [])\n",
    "        self.value_losses = checkpoint.get('value_losses', [])\n",
    "        self.entropies = checkpoint.get('entropies', [])\n",
    "\n",
    "        print(f\"Model and metrics loaded from {load_dir}\")\n",
    "\n",
    "    def moving_average(self, data, window_size=10):\n",
    "        \"\"\"Compute moving average for smooth plotting.\"\"\"\n",
    "        cumsum = np.cumsum(np.insert(data, 0, 0))\n",
    "        return (cumsum[window_size:] - cumsum[:-window_size]) / window_size\n",
    "\n",
    "    def plot_metrics(self, window_size=100):\n",
    "        episodes = range(len(self.episode_rewards))\n",
    "\n",
    "        # Calculate moving averages\n",
    "        ma_rewards = self.moving_average(self.episode_rewards, window_size)\n",
    "        ma_policy_losses = self.moving_average(self.policy_losses, window_size)\n",
    "        ma_value_losses = self.moving_average(self.value_losses, window_size)\n",
    "        ma_entropies = self.moving_average(self.entropies, window_size)\n",
    "\n",
    "        # Plotting\n",
    "        fig, axs = plt.subplots(4, 1, figsize=(10, 15))\n",
    "\n",
    "        # Total Rewards per Episode\n",
    "        axs[0].plot(episodes, self.episode_rewards, label=\"Total Reward\", color='blue', alpha=0.5)\n",
    "        axs[0].plot(episodes[window_size-1:], ma_rewards, label=f\"Moving Avg ({window_size})\", color='blue')\n",
    "        axs[0].set_title(\"Total Rewards per Episode\")\n",
    "        axs[0].set_xlabel(\"Episode\")\n",
    "        axs[0].set_ylabel(\"Reward\")\n",
    "        axs[0].legend()\n",
    "\n",
    "        # Policy Loss\n",
    "        axs[1].plot(episodes, self.policy_losses, label=\"Policy Loss\", color='red', alpha=0.5)\n",
    "        axs[1].plot(episodes[window_size-1:], ma_policy_losses, label=f\"Moving Avg ({window_size})\", color='red')\n",
    "        axs[1].set_title(\"Policy Loss per Episode\")\n",
    "        axs[1].set_xlabel(\"Episode\")\n",
    "        axs[1].set_ylabel(\"Policy Loss\")\n",
    "        axs[1].legend()\n",
    "\n",
    "        # Value Loss\n",
    "        axs[2].plot(episodes, self.value_losses, label=\"Value Loss\", color='green', alpha=0.5)\n",
    "        axs[2].plot(episodes[window_size-1:], ma_value_losses, label=f\"Moving Avg ({window_size})\", color='green')\n",
    "        axs[2].set_title(\"Value Loss per Episode\")\n",
    "        axs[2].set_xlabel(\"Episode\")\n",
    "        axs[2].set_ylabel(\"Value Loss\")\n",
    "        axs[2].legend()\n",
    "\n",
    "        # Entropy\n",
    "        axs[3].plot(episodes, self.entropies, label=\"Entropy\", color='purple', alpha=0.5)\n",
    "        axs[3].plot(episodes[window_size-1:], ma_entropies, label=f\"Moving Avg ({window_size})\", color='purple')\n",
    "        axs[3].set_title(\"Entropy per Episode\")\n",
    "        axs[3].set_xlabel(\"Episode\")\n",
    "        axs[3].set_ylabel(\"Entropy\")\n",
    "        axs[3].legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def train(self, max_episode=5000):\n",
    "        mean_total_reward = 0\n",
    "        mean_length = 0\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        for i in range(max_episode):\n",
    "            with torch.no_grad():\n",
    "                mb_states, mb_actions, mb_old_a_logps, mb_values, mb_returns, mb_advs, mb_rewards = self.run_episode()\n",
    "\n",
    "            # Train PPO agent\n",
    "            pg_loss, v_loss, ent = self.ppo_update(mb_states, mb_actions, mb_values, mb_advs, mb_returns, mb_old_a_logps)\n",
    "\n",
    "            # Track metrics\n",
    "            self.episode_rewards.append(mb_rewards.sum())\n",
    "            self.policy_losses.append(pg_loss)\n",
    "            self.value_losses.append(v_loss)\n",
    "            self.entropies.append(ent)\n",
    "\n",
    "            mean_total_reward += mb_rewards.sum()\n",
    "            mean_length += len(mb_states)\n",
    "\n",
    "            # Logging\n",
    "            print(\"[Episode {:4d}] total reward = {:.6f}, length = {:d}\".format(i, mb_rewards.sum(), len(mb_states)))\n",
    "\n",
    "\n",
    "            if i > 100 and sum(self.episode_rewards[-5:])>1200:\n",
    "                print(\"Training done\")\n",
    "                self.save_metrics()\n",
    "\n",
    "\n",
    "            if i % 200 == 0:\n",
    "                print(\"\\n[{:5d} / {:5d}]\".format(i, max_episode))\n",
    "                print(\"----------------------------------\")\n",
    "                print(\"actor loss = {:.6f}\".format(pg_loss))\n",
    "                print(\"critic loss = {:.6f}\".format(v_loss))\n",
    "                print(\"entropy = {:.6f}\".format(ent))\n",
    "                print(\"mean return = {:.6f}\".format(mean_total_reward / 200))\n",
    "                print(\"mean length = {:.2f}\".format(mean_length / 200))\n",
    "                print(\"\\nSaving the model ... \", end=\"\")\n",
    "                self.save_metrics()\n",
    "                mean_total_reward = 0\n",
    "                mean_length = 0\n",
    "\n",
    "    def test(self, test_episodes=10):\n",
    "        total_reward = 0\n",
    "\n",
    "        for episode in range(test_episodes):\n",
    "            state = self.env.reset()\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                state_tensor = torch.tensor(np.expand_dims(state, axis=0), dtype=torch.float32, device=self.device)\n",
    "                action, _ = self.policy_net(state_tensor)\n",
    "                action = action.cpu().numpy()[0]\n",
    "                state, reward, done, _ = self.env.step(action)\n",
    "                episode_reward += reward\n",
    "\n",
    "            total_reward += episode_reward\n",
    "            print(f\"Episode {episode + 1} Reward: {episode_reward:.6f}\")\n",
    "\n",
    "        avg_reward = total_reward / test_episodes\n",
    "        print(f\"Average Reward over {test_episodes} episodes: {avg_reward:.6f}\")\n",
    "        return avg_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_agent = PPO(env, policy_net, value_net, s_dim=s_dim, a_dim=a_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and metrics loaded from gaussian_ppo_bipedal_2.pt\n"
     ]
    }
   ],
   "source": [
    "ppo_agent.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
