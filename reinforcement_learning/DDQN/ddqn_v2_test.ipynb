{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "import copy\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import psutil\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janak/Documents/Pytorch_CPU/venv/lib/python3.10/site-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/home/janak/Documents/Pytorch_CPU/venv/lib/python3.10/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "# Get current process ID\n",
    "pid = os.getpid()\n",
    "process = psutil.Process(pid)\n",
    "env = gym.envs.make(\"LunarLander-v2\",render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    def __init__(self, n_state, n_action, n_hidden=50, lr=0.05, device='cpu'):\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "        self.device = device\n",
    "        self.model = torch.nn.Sequential(\n",
    "                        torch.nn.Linear(n_state, n_hidden),\n",
    "                        torch.nn.ReLU(),\n",
    "                        torch.nn.Linear(n_hidden, n_hidden),\n",
    "                        torch.nn.ReLU(),\n",
    "                        torch.nn.Linear(n_hidden, n_hidden),\n",
    "                        torch.nn.ReLU(),\n",
    "                        torch.nn.Linear(n_hidden, n_action)\n",
    "                )\n",
    "\n",
    "\n",
    "        self.model_target = copy.deepcopy(self.model)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr)\n",
    "\n",
    "    def update(self, s, y):\n",
    "        \"\"\"\n",
    "        Update the weights of the DQN given a training sample\n",
    "        @param s: state\n",
    "        @param y: target value\n",
    "        \"\"\"\n",
    "        s = np.array(s)\n",
    "        y_pred = self.model(torch.Tensor(s))\n",
    "\n",
    "        loss = self.criterion(y_pred, Variable(torch.Tensor(np.array(y))))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "    def predict(self, s):\n",
    "        \"\"\"\n",
    "        Compute the Q values of the state for all actions using the learning model\n",
    "        @param s: input state\n",
    "        @return: Q values of the state for all actions\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            return self.model(torch.Tensor(np.array(s)))\n",
    "\n",
    "    def target_predict(self, s):\n",
    "        \"\"\"\n",
    "        Compute the Q values of the state for all actions using the target network\n",
    "        @param s: input state\n",
    "        @return: targeted Q values of the state for all actions\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            return self.model_target(torch.Tensor(np.array(s)))\n",
    "\n",
    "    def replay(self, memory, replay_size, gamma):\n",
    "        \"\"\"\n",
    "        Experience replay with target network\n",
    "        @param memory: a list of experience\n",
    "        @param replay_size: the number of samples we use to update the model each time\n",
    "        @param gamma: the discount factor\n",
    "        \"\"\"\n",
    "        if len(memory) >= replay_size:\n",
    "            replay_data = random.sample(memory, replay_size)\n",
    "\n",
    "            states = []\n",
    "            td_targets = []\n",
    "            for state, action, next_state, reward, is_done in replay_data:\n",
    "                states.append(state)\n",
    "                q_values = self.predict(state).tolist()\n",
    "                if is_done:\n",
    "                    q_values[action] = reward\n",
    "                else:\n",
    "                    q_values_next = self.target_predict(next_state).detach()\n",
    "\n",
    "                    q_values[action] = reward + gamma * torch.max(q_values_next).item()\n",
    "\n",
    "                td_targets.append(q_values)\n",
    "\n",
    "            self.update(states, td_targets)\n",
    "\n",
    "    def copy_target(self):\n",
    "        self.model_target.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def name(self):\n",
    "      return \"Model C\"\n",
    "\n",
    "    def save_model(self, file_path):\n",
    "        torch.save(self.model.state_dict(), file_path)\n",
    "\n",
    "    def load_model(self, file_path):\n",
    "        self.model.load_state_dict(torch.load(file_path, map_location=self.device))\n",
    "        self.model_target = copy.deepcopy(self.model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_epsilon_greedy_policy(estimator, epsilon, n_action):\n",
    "    def policy_function(state):\n",
    "        if random.random() < epsilon:\n",
    "            return random.randint(0, n_action - 1)\n",
    "        else:\n",
    "            q_values = estimator.predict(state)\n",
    "            return torch.argmax(q_values).item()\n",
    "    return policy_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "n_state = env.observation_space.shape[0]\n",
    "n_action = env.action_space.n\n",
    "n_hidden = 62\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "dqn = DQN(n_state,n_action,n_hidden,lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_path = '../DDQN/double_dqn_lunar_lander.pth'\n",
    "\n",
    "dqn.model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=8, out_features=62, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=62, out_features=62, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=62, out_features=62, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=62, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(dqn.model)\n",
    "\n",
    "count = 0\n",
    "max_reward = 0\n",
    "episodes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janak/Documents/Pytorch_CPU/venv/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: -134.22681434268281\n",
      "Total reward: 245.04391343950945\n",
      "Total reward: -286.1313175794962\n",
      "Total reward: 272.0218405333896\n",
      "Total reward: -287.0938623949765\n",
      "Total reward: -94.34661989757164\n",
      "Total reward: 50.14712166099059\n",
      "Total reward: 221.18472407714304\n",
      "Total reward: 265.9458286437234\n",
      "Total reward: 290.72607694002954\n",
      "50.0\n",
      "290.72607694002954\n"
     ]
    }
   ],
   "source": [
    "for i in range(episodes):\n",
    "    total_reward = 0\n",
    "    \n",
    "    state = env.reset()\n",
    "    is_done = False\n",
    "    while not is_done:\n",
    "        q_values = dqn.predict(state)\n",
    "        # print(q_values)\n",
    "        action = torch.argmax(q_values).item()\n",
    "        next_state,reward,is_done,_ = env.step(action)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "    \n",
    "    print(f\"Total reward: {total_reward}\")\n",
    "    if total_reward > max_reward:\n",
    "        max_reward = total_reward\n",
    "    if total_reward>200:\n",
    "        count += 1\n",
    "    time.sleep(2)\n",
    "\n",
    "print((count/episodes)*100)\n",
    "print(max_reward)\n",
    "# Close the environment when done\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
